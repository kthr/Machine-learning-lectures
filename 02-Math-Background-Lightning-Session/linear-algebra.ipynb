{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "db7323b1-1a66-47cb-bce7-833fc5728820"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Algebra\n",
    "   \n",
    "   This is going to be a recap of the main concepts of linear algebra that we will need for Machine Learning. It is not a fully structured and coherent lecture, but more like a collection of ideas and concepts. I hope it will still be useful...\n",
    "   \n",
    "   ### topics\n",
    "   \n",
    "  - What and why? \n",
    "  - Scalars, Vectors, Matrices\n",
    "  - What can we do with them?\n",
    "  - Systems of Linear Equations\n",
    "  - The Transpose\n",
    "  - Matrix operations\n",
    "  - Linear Dependence and Span\n",
    "  - Norms\n",
    "  - Special Kinds of Matrices and Vectors\n",
    "  - Eigendecomposition\n",
    "  - Singular Value Decomposition\n",
    "  - The Moore-Penrose Pseudoinverse\n",
    "  - The Trace Operator\n",
    "  - The Determinant\n",
    "  - (Example: PCA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c9563bae-f44b-4b42-a984-d4c617609614"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Algebra\n",
    "\n",
    "## What and Why?\n",
    "- a branch of mathematics that deals with linear systems in n dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- systems of linear equations $a_1 x_1 + a_2 x_2 + ... + a_n x_n = b $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- linear functions $ (x_1, x_2, ..., x_n) \\mapsto a_1 x_1 + a_2 x_2 + ... + a_n x_n$\n",
    "- their representations through matrices and vector spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- maybe the most important basis for applied mathematics\n",
    "    - Geometry, \n",
    "    - Computer Graphics,\n",
    "    - Engineering,\n",
    "    - Quantum mechanics,\n",
    "    - Fourier series,\n",
    "    - **Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## History\n",
    "- from the study of determinants to solve systems of linear equations\n",
    "- Leibniz 1693\n",
    "- Cramer 1750\n",
    "- Gauss\n",
    "- Grassman 1844\n",
    "- Sylvester 1848, introduced the term _matrix_\n",
    "- Pasha 1882, the first book on _linear algebra_\n",
    "- Peano 1888, _vector spaces_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "040cc7c9-29fb-4bf8-a210-026ba8359fa1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scalars, Vectors, Matrices, and Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scalars\n",
    "\n",
    "- just a single number, e.g. $a \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vectors\n",
    "- an array of numbers, arranged in order\n",
    "- typically written as $\\vec{x}$ or $\\mathbf{x}$\n",
    "- n real elements then $\\mathbf{x} \\in \\mathbb{R} ^n$\n",
    "- $ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$\n",
    "- needs _one_ index to specify an element\n",
    "- think of vectors as identifying points in n-dimensional space\n",
    "    - each element giving the coordinate along an axis (dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "- a 2D array of numbers\n",
    "- upper-case variables $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- real-valued, $m$ rows and $n$ columns, then $\\mathbf{A} \\in \\mathbb{R}^{m \\times n} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- a specific element $A_{i,j}$ needs _two indices_ to identify member"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $ \\mathbf{A} = \\begin{bmatrix} a_{1,1} & a_{1,2}  \\\\ a_{2,1} & a_{2,2} \\\\ a_{3,1} & a_{3,2} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tensors\n",
    "- a 'generalization' of scalars, vectors, matrices: \n",
    "    - an array of numbers arranged in a grid with variable number of axes \n",
    "- e.g. $A_{i,j,k}$ \n",
    "    - would need _three_ indices per entry\n",
    "- this is oversimplified: see https://en.wikipedia.org/wiki/Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors\n",
    "- $ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$\n",
    "- think of vectors as identifying points in n-dimensional space\n",
    "    - each element giving the coordinate along an axis (dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adding Vectors\n",
    "- \"you can't add apples to oranges\" [Strang]\n",
    "- but you can add the vectors, the components stay separate\n",
    "    - $\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}$ and $\\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}$ add to $\\mathbf{v} + \\mathbf{w}= \\begin{bmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\end{bmatrix}$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![add apples to apples and oranges to oranges](img/add-apples-and-oranges.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Operations on vectors are usually done one component at a time!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Vector addition geometrically speaking..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Vector_addition.png/640px-Vector_addition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scalar Multiplication\n",
    "- the second important operation for vectors\n",
    "- we multiply each component (again) by a scalar value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $2\\mathbf{v}=  \\begin{bmatrix} 2 v_1 \\\\  2 v_2 \\end{bmatrix} $ or:  $-\\mathbf{v}=  \\begin{bmatrix} - v_1 \\\\  - v_2 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear Combinations\n",
    "- Linear algebra is built on these two operations (hence the name linear).\n",
    "- we can form *linear combinations* by multiplying vectors with scalars and adding the results:\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $c\\mathbf{v}+d\\mathbf{w}$ is the prototypic linear combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $1\\mathbf{v}+1\\mathbf{w}$ is the vector sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $1\\mathbf{v}-1\\mathbf{w}$ is the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $0\\mathbf{v}+0\\mathbf{w}$ *zero vector*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $c\\mathbf{v}+0\\mathbf{w}$ scaled vector in same direction as $\\mathbf{v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- all possible linear combinations of n vectors typically fill the entire n-dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear combinations: a graphical example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/3/3c/Linear_combination_in_2D_space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representing Vectors\n",
    "- as two numbers (or n numbers in n-dimensional space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- as an arrow from the origin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- as a point in the plane (or n-dimensional space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- a vector in n dimensions has n components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{v}+\\mathbf{w}=(v_1+v_2, w_1+w_2)$ and $c\\mathbf{v}=(c v_1, c v_2)$: one component at a time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- linear combination of three vectors is: $c \\mathbf{u}+ d\\mathbf{v}+e\\mathbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- all combinations of three (independent) vectors fill the whole $\\mathbb{R}^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lengths, Angles, and the Dot (inner) Product\n",
    "- in 2D: between vectors $x$ and $y$ of the same dimensionality: $v \\cdot w = v_1 w_1 + v_2 w_2$\n",
    "- in nD: the same...\n",
    "- commutative: the order doesn't matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![source: betterexplained.com/](https://betterexplained.com/wp-content/uploads/dotproduct/dot_product_rotation.png)\n",
    "- source: betterexplained.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### length of vectors\n",
    "- length of $\\mathbf{v}$ is square root of inner product of $\\mathbf{v}$ with itself: $\\lVert \\mathbf{v} \\rVert = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} $\n",
    "- a *unit vector* has length of 1\n",
    "    - $\\mathbf{u} = \\frac{\\mathbf{v}}{\\lVert \\mathbf{v} \\rVert}$ is the unit vector in the direction of $\\mathbf{v}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [1 1]\n",
      "length of v: 1.41421356237\n"
     ]
    }
   ],
   "source": [
    "v=np.array([1,1])\n",
    "print(\"v: \"+str(v))\n",
    "print(\"length of v: \" + str(np.sqrt(np.dot(v,v))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [1 1]\n",
      "length of v: 1.41421356237\n",
      "unit vector in direction of v: [ 0.70710678  0.70710678]\n",
      "length of unit vector: 1.0\n"
     ]
    }
   ],
   "source": [
    "v=np.array([1,1])\n",
    "norm_v = np.sqrt(np.dot(v,v))\n",
    "u = v / norm_v \n",
    "print(\"v: \"+str(v))\n",
    "print(\"length of v: \"+str(norm_v))\n",
    "print(\"unit vector in direction of v: \"+str(u))\n",
    "print(\"length of unit vector: \"+str(np.sqrt(np.dot(u,u))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### angle between vectors\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dot_Product.svg/220px-Dot_Product.svg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the dot product is 0 when v is perpendicular to w!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.array([1,0]),np.array([0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- more generally, if $\\mathbf{a}$ and $\\mathbf{b}$ are nonzero vectors: \n",
    "    - $\\frac{\\mathbf{a}\\cdot \\mathbf{b}}{\\lVert \\mathbf{a} \\rVert \\lVert \\mathbf{b} \\rVert} = \\cos \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- instead of $\\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix} \\cdot \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}$ we often write $\\begin{bmatrix} a_1 & a_2 \\end{bmatrix} \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}$, i.e. $\\mathbf{a}^\\top \\mathbf{b}$, hence the name: *inner product*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- dot (inner) product multiplies each component and adds the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- length of a vector is the square root of the dot product with itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- a vector divided by its length gives the unit vector in the same direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the dot product is 0 when the vectors are perpendicular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the cosine of the angle between vectors is the dot product normalized by the multiplied lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "- a 2D array of numbers\n",
    "- $ \\mathbf{A} = \\begin{bmatrix} a_{1,1} & a_{1,2}  \\\\ a_{2,1} & a_{2,2} \\\\ a_{3,1} & a_{3,2} \\end{bmatrix}$\n",
    "- matrices represent linear transformations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiplying Matrices and Vectors\n",
    "- $\\mathbf{y}=\\mathbf{A}\\mathbf{x}$, \n",
    "    - $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, \n",
    "    - $\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$, \n",
    "    - then $\\mathbf{y} \\in \\mathbb{R}^{m \\times 1}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $y_{i,1}=\\sum_{k} A_{i,k} x_{k,1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **y is a linear combination of A's columns** (weights defined by x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From Vectors to Matrices\n",
    "- $u = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0\\end{bmatrix}, v = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1\\end{bmatrix}, w = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1\\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the combination $c\\mathbf{u} + d\\mathbf{v} + e\\mathbf{w} = \\begin{bmatrix}c \\\\d-c \\\\ e-d \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- let's rewrite (stack) the vectors into matrix and multiply with the vector of the coefficients: \n",
    "    - $ \\begin{bmatrix}1 & 0 & 0  \\\\ -1 & 1 & 0 \\\\ 0 & -1 & 1 \\end{bmatrix} \\begin{bmatrix}c \\\\d \\\\ e \\end{bmatrix} = \\begin{bmatrix}c \\\\d-c \\\\ e-d \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It is useful to interpret matrices as a collection of (column) vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From Vectors to Matrices II\n",
    "- $\\mathbf{y}=\\mathbf{A}\\mathbf{x}$:\n",
    "- the matrix A acts on the vector x\n",
    "    - the result y is *a linear combination of the columns* of A: Ax = y\n",
    "    - this is a linear system of equations! We will get back to this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\begin{bmatrix} 1 & 2  \\\\ 3 & 4 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(np.array([[1,2],[3,4]]),np.array([[1], [2]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## System of Linear Equations\n",
    "\n",
    "- $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$\n",
    "- $\\mathbf{A}\\in\\mathbb{R}^{m \\times n}$ the coefficients, $\\mathbf{b}\\in\\mathbb{R}^{m}$ a known vector, and $\\mathbf{x}\\in\\mathbb{R}^{n}$ a vector of unknown variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- each row of $\\mathbf{A}$ and entry of $\\mathbf{b}$ provide a constraint, e.g. $\\mathbf{A}_{1,1}x_1+\\mathbf{A}_{1,2}x_2+...+\\mathbf{A}_{1,n}x_n = b_1$\n",
    "    - i.e. it defines a (hyper)plane in n-dimensional space\n",
    "    - we get a solution if all of them intersect in the same place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How to solve such equations?\n",
    "- We will need some special matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identity Matrix\n",
    "\n",
    "- $\\mathbf{I}_n\\mathbf{x}=\\mathbf{x}$, $\\forall \\mathbf{x}\\in\\mathbb{R}^n$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- so in 2D: $\\mathbf{I}_2=\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- it is the neutral element (just like the 1 in scalar multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inverse matrix\n",
    "\n",
    "- $\\mathbf{A}^{-1}$ is the matrix inverse of $\\mathbf{A}$, defined as:\n",
    "- $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the inverse of each element (just as 1/x in scalar multiplication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### solving equations  I\n",
    "\n",
    "- $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{I}_{n}\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$\n",
    "\n",
    "- several methods for finding inverse if it exists\n",
    "- in practice the inverse is usually not computed directly\n",
    "    - we often use iterative methods such as Gaussian Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- Matrix times vector: Ax = combination of columns of A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The solution to $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ is $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$, when $\\mathbf{A}$ is invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The inverse matrix gives $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}_n$ and $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- if $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ for a nonzero vector x then A has no inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## the Transpose\n",
    "\n",
    "- important and much simpler as inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- mirror image of the matrix across main diagonal: $(\\mathbf{A}^\\top)_{i,j}=A_{j,i}$\n",
    "- columns become rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3],[4,5,6]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "print(np.transpose(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- vectors can be seen as 1D matrices, i.e:  $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$ can be written as $\\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix}^\\top$ \n",
    "- a scalar is a 1x1 matrix, i.e. $a^\\top = a $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $\\mathbf{A}\\mathbf{x}$ combines the columns of A while $\\mathbf{x^\\top}\\mathbf{A^\\top}$ combines the rows of $\\mathbf{A^\\top}$: \n",
    "- the same combination of the same vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- more mathematically speaking: $\\mathbf{A^\\top}$ is the matrix that makes those two products equal for every x and y:\n",
    "- $(\\mathbf{A}\\mathbf{x})^\\top \\mathbf{y} = \\mathbf{x}^\\top (\\mathbf{A}^\\top \\mathbf{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- that explains the name \"inner products\":\n",
    "- $^\\top$ inside: the dot or inner product: $\\mathbf{x}^\\top \\mathbf{y}, (1 \\times n)(n \\times 1)$\n",
    "- $^\\top$ outside: the rank or outer product: $\\mathbf{x}\\mathbf{y}^\\top, (n \\times 1)(1 \\times n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Inner products are everywhere! we often compute weighted sums:\n",
    "    - work = movements forces: $ \\mathbf{x}^\\top \\mathbf{f}$\n",
    "    - income = quantities prices: $ \\mathbf{q}^\\top \\mathbf{p}$\n",
    "    - linear model = weights features: $ \\mathbf{w}^\\top \\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- the transpose puts rows of $\\mathbf{A}$ into the columns of $\\mathbf{A}^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the dot product is $\\mathbf{x}^\\top \\mathbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $(\\mathbf{A} \\mathbf{x})^\\top \\mathbf{y} = \\mathbf{x}^\\top (\\mathbf{A}^\\top \\mathbf{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (some) Matrix computations\n",
    "### Matrix addition \n",
    "\n",
    "- $\\mathbf{C}=\\mathbf{A}+\\mathbf{B}$, where $\\mathbf{C}_{i,j}= \\mathbf{A}_{i,j}+\\mathbf{B}_{i,j}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### scalar multiplication\n",
    "\n",
    "- $\\mathbf{D} = a \\mathbf{B}$, where $\\mathbf{D}_{i,j} = a \\mathbf{B}_{i,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiplying matrices\n",
    "- $\\mathbf{C}=\\mathbf{A}\\mathbf{B}$, defined only if $\\mathbf{A}$ has as many columns as $\\mathbf{B}$ has rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{B} \\in \\mathbb{R}^{n \\times p}$, then $\\mathbf{C} \\in \\mathbb{R}^{m \\times p}$:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $C_{i,j}=\\sum_{k} A_{i,k} B_{k,j}$\n",
    "- same principle: each column of C is a linear combination of the columns of A (defined by column vectors of B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Multiplying Matrices and Vectors\n",
    "- $\\mathbf{y}=\\mathbf{A}\\mathbf{x}$, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$, then $\\mathbf{y} \\in \\mathbb{R}^{m \\times 1}$:\n",
    " - $y_{i,1}=\\sum_{k} A_{i,k} x_{k,1}$\n",
    "- y is a linear combination of A's columns (weights defined by x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### some useful Properties\n",
    "\n",
    "- distributive: $\\mathbf{A}\\left(\\mathbf{B}+\\mathbf{C}\\right) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- associative: $\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- *not* commutative: $\\mathbf{A}\\mathbf{B}$ not always the same as $\\mathbf{B}\\mathbf{A} $ \n",
    "    - but the inner vector product is: $\\mathbf{x}^\\top \\mathbf{y} = \\mathbf{y}^\\top \\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- transpose of product: $(\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top\\mathbf{A}^\\top$\n",
    "- many more properties exist. Consult a textbook..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "- a little proof: \n",
    "    - $\\mathbf{x}^\\top \\mathbf{y} = (\\mathbf{x}^\\top \\mathbf{y})^\\top $, since it is a scalar.\n",
    "    - $(\\mathbf{x}^\\top \\mathbf{y})^\\top  = \\mathbf{y}^\\top \\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Dependence and Span\n",
    "### Solving Equations (again)\n",
    "- $\\mathbf{A}^{-1}$ only exists if $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ has exactly one solution for each b\n",
    "    - all the hyperplanes need to intersect in a common point\n",
    "    - but it could have no solution or infinitely many solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- columns of A define vectors (directions)\n",
    "- Can we reach $\\mathbf{b}$ by taking steps of certain size along these directions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{x}$ specifies these step lengths: $\\mathbf{A}\\mathbf{x} = \\sum_{i}x_i \\mathbf{A}_{:,i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the *span* of a set of vectors is the set of all points reachable by linear combinations of those vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- $\\mathbf{A}^{-1}$ only exists if $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ has exactly one solution for each b\n",
    "    - all the hyperplanes need to intersect in a common point\n",
    "    - but it could have no solution or infinitely many solutions.\n",
    "    - it cannot have more than 1 but less than infinitely many: $\\alpha\\mathbf{x}+(1-\\alpha)\\mathbf{y}$\n",
    "- columns of A define vectors (directions)\n",
    "- this operation is a *linear combination*:\n",
    "    - reminder: a linear combination of a set of vectors $\\{\\mathbf{v}^{(1)},\\mathbf{v}^{(1)},...,\\mathbf{v}^{(n)}\\}$ is given by multiplying each $\\mathbf{v}^{(i)}$ by a scalar and sum over results: $\\sum_{i} c_i \\mathbf{v}^{(i)} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solving Equations II\n",
    "- $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ has a solution if $\\mathbf{b}$ is in the span of the columns of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the span this is called the *column space* of $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- to have solutions for all $\\mathbf{b} \\in \\mathbb{R}^m $ the column space of $\\mathbf{A}$ needs to be all of  $\\mathbb{R}^m $.\n",
    "- i.e. $\\mathbf{A}$ needs at least $m$ columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- e.g. a $3 \\times 2$ matrix: $\\mathbf{b}$ is 3D but $\\mathbf{x}$ is only 2D. The column space has only 2 basis vectors, it can only trace out a 2-D plane. There are only solutions for $\\mathbf{b}$ on that plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear Dependence\n",
    "- $n \\geq m$ (more columns than rows) is only necessary but not sufficient as columns could be redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\begin{bmatrix} 0 & 0 \\\\ 1 & 1 \\end{bmatrix}$ has two columns, but column space is a single line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- this is called *linear dependence*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A set of vectors is linear *independent* if no vector (in the set) is a linear combination of the other vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Thus, to have a column space to encompass $\\mathbb{R}^{m}$, the matrix must have at least $m$ linearly independent columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- $\\mathbf{A}$ is invertible if it has at most one solution for each value of $\\mathbf{b}$, i.e. it has at most $m$ columns.\n",
    "    - i.e. the matrix must be square ($m=n$) and all columns must be linearly independent. Otherwise it is *singular*.\n",
    "    - it is still possible to solve the equation if $\\mathbf{A}$ is singular or non-square but not with matrix inversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Least Squares Solutions and the Transpose\n",
    "\n",
    "- When $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ has no solution, multiply by $\\mathbf{A}^\\top$ and solve $\\mathbf{A}^\\top \\mathbf{A}\\mathbf{x} =  \\mathbf{A}^\\top \\mathbf{b}$\n",
    "- Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- short answer: \n",
    "- the columns of $\\mathbf{A}$ span only a small part of the m-dimensional space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- All the potential $\\mathbf{b}$s that would admit a solution to $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ lie on a subspace (hyperplane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For all others $\\mathbf{b}$s we will have a remaining error term $\\mathbf{e} = \\mathbf{b}- \\mathbf{A}\\mathbf{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We want to look for the point closest to $\\mathbf{b}$ in this plane\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the error $(\\mathbf{b}-\\mathbf{A}\\mathbf{x})$ makes a right angle with all (column) vectors $\\mathbf{a}_i$ of $\\mathbf{A}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the inner product $\\mathbf{a}_i^\\top(\\mathbf{b}-\\mathbf{A}\\mathbf{x})$ is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- this can be written as a linear system $\\mathbf{A}^\\top(\\mathbf{b}-\\mathbf{A}\\mathbf{x})=\\mathbf{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{A}^\\top \\mathbf{A}\\mathbf{x} =  \\mathbf{A}^\\top \\mathbf{b}$ gives us the solution on the subspace that it closest to the desired b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- $\\mathbb{R}^n$ contains all column vectors with n real components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- a set of vectors is independent if no vector is a linear combination of the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the columns of a matrix are independent if $\\mathbf{x}=\\mathbf{0}$ is the only solution to $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- a set of vectors *span* a space if their combinations fill that space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the combinations of columns of A form the *column space* \n",
    "    - the column space is *spanned* by the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ has a solution exactly when $\\mathbf{b}$ is in the column space of $\\mathbf{A}$\n",
    "    - otherwise we can obtain a least squares solution by using the transpose of A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Norms\n",
    "- we often need to measure the size of a vector, we usually use a function called a *norm* ($L^p$ norm): \n",
    "    - $||\\mathbf{x}||_{p} = \\left( \\sum_{i} \\left|x_i\\right|^p \\right) ^ {\\frac{1}{p}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- norms map vectors to non-negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- measure \"distance to the origin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- formally, a norm is any function $f$ satisfying:\n",
    "    - $f(\\mathbf{x}=0) \\implies \\mathbf{x}=\\mathbf{0}$\n",
    "    - $f(x+y)\\leq f(x) + f(y)$\n",
    "    - $\\forall \\alpha \\in \\mathbb{R}, f(\\alpha \\mathbf{x}) = \\left|\\alpha \\right|f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The $L^2$ norm is the \"standard\" Euclidean norm and is often denoted by $\\lVert \\mathbf{x} \\rVert$. It gives the Euclidean distance from the origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can also measure the size of $\\mathbf{x}$ using the *squared* $L^2$ norm: $\\mathbf{x}^\\top \\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- easier to work with mathematically and computationally:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- its derivatives with respect to each element depend only on that element, while for $L^2$ it depends on the entire vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In an optimization context, squared $L^2$ is not so desirable, because it increases slowly near 0. We often want to discriminate between *near zero* and *zero*. \n",
    "- For this the $L^1$ norm is used: $$ \\lVert \\mathbf{x} \\rVert _{1} = \\sum_i \\left| x_i \\right|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sometimes, the size of a vector is measured by counting the number of nonzero elements. This is not a norm (scaling property)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another common form is the $L^\\infty$ or *max norm*: $$ \\lVert \\mathbf{x} \\rVert _{\\infty} = \\max_i \\left| x_i \\right| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To measure the size of a matrix, the most common is the *Frobenius norm* $$\\lVert \\mathbf{A} \\rVert _{F} = \\sqrt{\\sum_{i,j} A^2_{i,j} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Special Kinds of Matrices and Vectors\n",
    "### Diagonal Matrices\n",
    "- $D_{i,j}=0$,  $\\forall i\\neq j$\n",
    "- written as $diag(\\mathbf{v})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- computationally very efficient: \n",
    "    - multiplication by a vector: $diag(\\mathbf{v}) \\mathbf{x} = \\mathbf{v}\\circ \\mathbf{x}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the inverse: $diag(\\mathbf{v})^{-1} = diag([\\frac{1}{v_1},...,\\frac{1}{v_n}]^\\top)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- we often derive machine learning methods in terms of arbitrary matrices, but obtain less expensive (and less descriptive) approximations by restricting to diagonal matrices.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Orthogonal Matrices\n",
    "- reminder: a *unit vector* is: $\\lVert \\mathbf{x} \\rVert = 1$\n",
    "- reminder II: $\\mathbf{x}$ and $\\mathbf{y}$ are *orthogonal* if $\\mathbf{x}^\\top \\mathbf{y} = 0$\n",
    "- if vectors are _orthogonal_ and of _unit length_, they are called _orthonormal_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An *orthogonal matrix* is a square matrix with mutually orthonormal columns (and rows): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $ \\mathbf{A}^\\top \\mathbf{A} = \\mathbf{A} \\mathbf{A}^\\top = \\mathbf{I}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\implies \\mathbf{A}^{-1} = \\mathbf{A}^\\top$\n",
    "- inverse is very cheap to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " - 90 degree angle between them\n",
    "    - in $\\mathbb{R}^n$ at most $n$ vectors can be orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Trace Operator\n",
    "- gives the sum of the diagonal elements: $Tr(\\mathbf{A})=\\sum_{i}A_{i,i} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- remember: it is the sum of the eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- some operations are difficult to specify without summation notation can be written in matrix products and trace operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- e.g. $\\lVert \\mathbf{A} \\rVert = \\sqrt{Tr(\\mathbf{A}\\mathbf{A}^\\top)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- trace is invariant to transposition $Tr(\\mathbf{A})=Tr(\\mathbf{A}^\\top)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $Tr(\\mathbf{A}\\mathbf{B}\\mathbf{C})=Tr(\\mathbf{C}\\mathbf{A}\\mathbf{B})=Tr(\\mathbf{B}\\mathbf{C}\\mathbf{A})$\n",
    "    - invariant to cyclic permutations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Determinant\n",
    "- the determinant of a square matrix $\\det(A)$ is a function from matrices to real scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- absolute value of determinant measures how multiplication with $\\mathbf{A}$ expands or contracts space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- if it is 1 , the transformation is volume preserving\n",
    "    - e.g. interesting for evaluating deformation fields in registrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- if it is 0, all space is contracted (at least in one dimension)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- remember: it is equal to the product of eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigendecomposition\n",
    "- mathematical objects can be understood better by finding universal properties independent of representation\n",
    "    - integer decomposition into prime factors: \n",
    "        - 12 vs 1100\n",
    "        - but 12 = 2 x 2 x 3 will always be true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- we can discover functional properties of matrices in a similar way: *Eigendecomposition*\n",
    "- Matrix -> set of eigenvectors and eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- *eigenvector* of $\\mathbf{A}$ is a (non-zero) vector $\\mathbf{v}$ such that multiplication with $\\mathbf{A}$ only changes the length of $\\mathbf{v}$ but not the orientation: \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $ \\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\lambda$ (a scalar) is the *eigenvalue* corresponding to the eigenvector $\\mathbf{v}$\n",
    "- we usually only consider *unit* eigenvectors ($\\alpha \\mathbf{v}$ will also be an eigenvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- If $\\mathbf{A}$ has n linearly independent eigenvectors, we can concatenate them into a matrix $\\mathbf{V}$ with one eigenvector per column: $\\mathbf{V} = \\begin{bmatrix} \\mathbf{v}^1 & ... & \\mathbf{v}^n \\end{bmatrix}$ and the eigenvalues into a vector $\\mathbf{\\lambda} = \\begin{bmatrix} \\mathbf{\\lambda}^1 & ... & \\mathbf{\\lambda}^\\top \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the eigendecomposition is then given by $$\\mathbf{A} = \\mathbf{V} diag(\\mathbf{\\lambda})\\mathbf{V}^{-1}$$\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110f524e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.linspace(0,np.pi*2,20)\n",
    "pl.plot(np.cos(t), np.sin(t), 'o', linewidth=1)\n",
    "pl.axes().set_xlim(-3,3)\n",
    "pl.axes().set_ylim(-3,3)\n",
    "pl.axes().set_aspect('equal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's construct a specfic matrix from eigenvectors and eigenvalues and see how it acts on vectors..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We start with defining the eigenvectors (direction) and the eigenvalues (stretch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st eigenvector:  [ 0.70710678  0.70710678] with eigenvalue:  2\n",
      "2nd eigenvector:  [ 0.70710678 -0.70710678] with eigenvalue:  1\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array((1,1))/np.sqrt(2)\n",
    "v2 = np.array((1,-1))/np.sqrt(2)\n",
    "lambda1 = 2\n",
    "lambda2 = 1\n",
    "print(\"1st eigenvector: \", v1, \"with eigenvalue: \", lambda1)\n",
    "print(\"2nd eigenvector: \", v2, \"with eigenvalue: \", lambda2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the matrix V composed of the eigenvectors: \n",
      " [[ 0.70710678  0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "V=np.column_stack((v1,v2))\n",
    "print(\"the matrix V composed of the eigenvectors: \\n\",V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the diagonal matrix containing the eigenvalues: \n",
      " [[2 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "diagLambda=np.diag(np.array((lambda1,lambda2)))\n",
    "print(\"the diagonal matrix containing the eigenvalues: \\n\",diagLambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From these matrices we can construct $$\\mathbf{A} = \\mathbf{V} diag(\\mathbf{\\lambda})\\mathbf{V}^{-1}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing V diag(Lambda) V^-1 \n",
      " [[ 1.5  0.5]\n",
      " [ 0.5  1.5]]\n"
     ]
    }
   ],
   "source": [
    "A=np.dot(np.dot(V,diagLambda), np.linalg.inv(V))\n",
    "print(\"computing V diag(Lambda) V^-1 \\n\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the eigendecomposition to check: \n",
      "\n",
      "the eigenvalues\n",
      "[ 2.  1.]\n",
      "the eigenvectors\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing the eigendecomposition to check: \\n\")\n",
    "evals, evecs = np.linalg.eig(A)\n",
    "print(\"the eigenvalues\")\n",
    "print(evals)\n",
    "print(\"the eigenvectors\")\n",
    "print(evecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original vectors on the unit circle\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11145bcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The original vectors on the unit circle\")\n",
    "t = np.linspace(0,np.pi*2,20)\n",
    "pl.plot(np.cos(t), np.sin(t), 'o' ,linewidth=1)\n",
    "pl.axes().set_xlim(-3,3)\n",
    "pl.axes().set_ylim(-3,3)\n",
    "pl.axes().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed vectors after multiplying by the matrix we constructed from the eigenvectors\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1112ec1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The transformed vectors after multiplying by the matrix we constructed from the eigenvectors\")\n",
    "pl.plot(res[0,:],res[1,:], 'o', linewidth=1)\n",
    "pl.axes().set_xlim(-3,3)\n",
    "pl.axes().set_ylim(-3,3)\n",
    "pl.axes().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The effect of $\\mathbf{A} \\mathbf{x}$\n",
    "- how are vectors (sampled from the unit circle) transformed by A?\n",
    "- we compute $\\mathbf{y}=\\mathbf{A}\\mathbf{x}$ and plot the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x=[np.array((np.cos(p), np.sin(p))) for p in np.linspace(0,np.pi*2,20)]\n",
    "res=np.dot(A,np.transpose(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed vectors after multiplying by the matrix A that we've constructed from the eigenvectors\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAEhCAYAAABWXVX4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFQ9JREFUeJzt3X+onud5H/DvVcVdDm7G+WOCxCcSMSwoK1aI6CEJeOxX\nk8mUsKgahZplo3Qg9kdZCq07Zw4t3Rzk4VEG3WA1JLSjxiWQRAmNS+LgQBeY3cixG8d2XEyhtU/L\n4m54rYlgtXLvDx3Z8s6Rznnf535/Pp8PCPy+Onrey7LPxfc893XfT7XWAgDAMD+06AIAANaBUAUA\n0IFQBQDQgVAFANCBUAUA0IFQBQDQweBQVVVvrao/qKo/rKpnqupXexQGMA96GNBLDT2nqqoqyc2t\ntVer6qYk30jy8dbaYz0KBJglPQzo5S1DL9CupLJXd1/etPvLiaLAStDDgF66zFRV1ZGqeirJ95I8\n0lp7vMd1AeZBDwN6GHynKklaa5eTvK+qNpN8oapua61959qvqapzSc4lyc033/xj73nPe3p8NLAi\nnnjiib9orR1ddB37OaiH6V8wboftX4NnqvZcsOqXk3y/tfYfr/c129vb7eLFi10/F1huVfVEa217\n0XUc5KAepn/B+By2f/XY/Xd096e7VNVGkg8n+e7Q6wLMgx4G9NJj+e8dSX6rqo7kSkj7bGvtdztc\nF2Ae9DCgix67/76d5FSHWgDmTg8DenGiOgBAB0IVAEAHQhUAQAdCFQBAB0IVAEAHQhUAQAdCFQBA\nB0IVAEAHQhUAQAdCFQBAB0IVAEAHQhUAQAdCFQBAB0IVAEAHQhUAQAdCFQBAB0IVAEAHQhUAQAdC\nFQBAB0IVAEAHQhUAQAdCFQBAB0IVAEAHQhUAQAdCFQBAB0IVAEAHQhUAQAdCFQBAB0IVAEAHQhUA\nQAdCFQBAB0IVAEAHQhUAQAeDQ1VVHauqr1fVs1X1TFV9vEdhAPOghwG9vKXDNV5L8guttW9V1duS\nPFFVj7TWnu1wbYBZ08OALgbfqWqt/Xlr7Vu7//xXSZ5LsjX0ugDzoIcBvXSdqaqqdyU5leTxntcF\nmAc9DBiiW6iqqh9J8rkkP99a+8t9fv9cVV2sqosvv/xyr48F6OJGPUz/Ag6jS6iqqptypRk92Fr7\n/H5f01p7oLW23VrbPnr0aI+PBejioB6mfwGH0WP3XyX5dJLnWmu/NrwkgPnRw4Beetypuj3JP0/y\nj6rqqd1fP9HhugDzoIcBXQw+UqG19o0k1aEWgLnTw4BenKgOANCBUAUA0IFQBQDQgVAFANCBUAUA\n0IFQBQDQgVAFANCBUAUA0IFQBQDQgVAFANDB4MfUAADzc+HJndz/lefzZ69cyi2bG7nr9ImcObW1\n6LKIUAUAK+PCkzv5xOefzqW/vpwk2XnlUj7x+aeTRLBaAkIVAKyI+7/y/OuB6qpLf30593/l+YlC\nlbtdsyFUAcCK+LNXLk30/n7c7Zodg+oAsCJu2dyY6P393OhuF8MIVQCwIu46fSIbNx1503sbNx3J\nXadPHPoaPe52sT/LfwAwQz3nl67+uSHXu2VzIzv7BKhJ7naxP6EKAGZkFvNLZ05tDZp9uuv0iTfV\nlEx+t4v9Wf4DgBlZxvmlM6e2cv7syWxtbqSSbG1u5PzZk4bUO3CnCgBmZFnnl4be7WJ/7lQBwIz0\n2K3H6hCqAGBGeuzWW6QLT+7k9vseza13fzm33/doLjy5s+iSlprlPwCYkR679RbFIaGTE6oAYIZW\ndX6p1yNxxsTyHwCwx7IO2S8zoQoA2MOQ/eSEKgBgj1Ufsl8EM1UAkL6Pk1kHqzxkvyhCFQCjZ6fb\n/lZ1yH5RLP8BMHrL+DgZVo9QBcDo2elGD0IVAKNnpxs9CFUAjJ6dbvRgUB2A0bPTjR66hKqq+kyS\njyT5Xmvtth7XBJgH/Yur7HRjqF7Lf7+Z5I5O12INeLI5K+Q3o38BHXS5U9Va+/2qelePa7H6hpz3\n4vA95k3/gsPTo2/MoDrdTXvey9UwtvPKpbS8Ecbc5QJYPD36YHMLVVV1rqouVtXFl19+eV4fSweT\nLuVNe96Lw/dYVvoX6NGHMbdQ1Vp7oLW23VrbPnr06Lw+loGm+clk2vNeHL7HstK/QI8+DMt/3NA0\nP5lMe96Lw/cAlpcefbAuoaqqHkryP5KcqKqXqupf9rguizfNTyZnTm3l/NmT2drcSCXZ2tzI+bMn\nDxxmdPgei6B/weHo0Qfrtfvvzh7XYfncsrmRnX0C1EE/mUxz3su0h+/ZjcIQ+hccjgNSD+ZEdW7o\nrtMn3nQ8QjLbn0wmDWNDjm8AYDIOSL0xM1Xc0LRLefNiNwoAy8KdKg60zD+Z2I0C42TZn2UkVI3Q\nOjWjaWe+gNVl2Z9lZflvZNbtRFy7UWB8LPuzrISqkVm3ZrTsM19Af5b9WVaW/0ZmHZvRMs98Af1Z\n9mdZuVM1Mk7EBVadZX+WlVA1MmNuRpM+GBpYTpb9WVaW/0ZmrCfi2i0E68WyP8tIqBqhMTajGw3o\nj+3vAoDZsPzHKKzjgD4Ay0WoYhQM6AMwa5b/GIV5PxgaYF188sLTeejxF3O5tRypyp0fOJZ7z5xc\ndFlLSahaQ+v0GJpexjqgDzDEJy88nd9+7E9ff325tddfC1Z7CVVrxi636xvjgD7AEA89/uJ13xeq\n9jJTtWbW7TE0ACzO5dYmen/shKo1Y5cbAL0cqZro/bETqtaMXW4A9HLnB45N9P7YCVVrZsyPoQGg\nr3vPnMzHPnj89TtTR6rysQ8eN091HQbV14xdbtOxYxJgf/eeOSlEHZJQtYbscpuMHZMA9CBUMXqe\nCwjz4Y4w606oYvTsmITZc0eYMTCozujZMQmz5ww9xkCoYvTsmITZc0eYMRCqGL0zp7Zy/uzJbG1u\npJJsbW7k/NmTliSgI3eEGQMzVSvO4GcfdkzCbN11+sSbZqoSd4RZP0LVCjP4CawKZ+gxBkLVCnMU\nALBK3BFm3ZmpWmEGPwFgeQhVK8zgJwAsD6FqhTkKAACWR5dQVVV3VNXzVfVCVd3d45oczFEA0Ice\nBvQweFC9qo4k+S9JPpzkpSTfrKovtdaeHXptDmbws69PXng6Dz3+Yi63liNVufMDxzydfc3pYUAv\nPXb/vT/JC621P06SqvqdJB9NoiGxUj554en89mN/+vrry629/lqwWmt6GKPjjMPZ6LH8t5XkxWte\nv7T7HqyUhx5/caL3WRt6GKNy9YzDnVcupeWNMw4vPLmz6NJW3twG1avqXFVdrKqLL7/88rw+Fg7t\ncmsTvc946F+sEw+3np0eoWonybFrXr9z9703aa090Frbbq1tHz16tMPHQl9HqiZ6n7VxYA/Tv1gn\nzjicnR6h6ptJ3l1Vt1bVDyf56SRf6nBdmKs7P3BsovdZG3oYo+KMw9kZHKpaa68l+bkkX0nyXJLP\nttaeGXpdmLd7z5zMxz54/PU7U0eq8rEPHjekvub0MMbGGYez0+XZf621h5M83ONaTMYOjr7uPXNS\niBohPYwx8XDr2fFA5RV2dQfH1YHDqzs4kvjmAOC6nHE4Gx5Ts8Ls4ACA5eFO1QqzgwNYFKMHsJc7\nVSvMDg5gERweCfsTqlaYHRzAIhg9gP1Z/lthdnAAi2D0APYnVK04Ozj6MB8Ch3fL5kZ29glQRg8Y\nO8t/jJ75EJiM0QPYn1DF6JkPgcmcObWV82dPZmtzI5Vka3Mj58+edHeX0bP8x+iZD4HJGT2Avdyp\nYvQcTQFAD0IVo2c+BIAeLP+tITvZJuNoCgB6EKrWjIcsT8d8CABDCVVr5kY72YQGgPVgRWI5CVVr\nxk42gPVmRWJ5GVRfM3ayAaw3Z+stL6FqzdjJBrDerEgsL8t/a8ZOtuszgwCsA89eXF5C1Rqyk20v\nMwjAurjr9Ik39bPEisSysPzHKJhBANaFZy8uL3eqGAUzCMA6sSKxnNypYhTsigRg1typGqExDmyb\nQYDJjbFXwBBC1ciMdWDbrkiYzFh7BQwhVI3MmB9jYwYBDm/MvQKmZaZqZAxsA4ehV8Dk3KkamXU8\nNM7cB/S3jr0CZs2dqpFZt8fYXJ372HnlUlremPu48OTOokuDlbZuvQLmQagamXU7NM6hnjAb69Yr\nYB4s/43QOg1sm/uA2VmnXgHzIFRxoGWeWTL3AcCysPzHDS37zJK5DwCWxaBQVVU/VVXPVNUPqmq7\nV1Esj3nPLF14cie33/dobr37y7n9vkcPDG/mPhhCDwN6Grr8950kZ5P8RodaWELTzixNs2Q47QnO\n5j4YQA9jLpZ5jIJ+Bt2paq0911qzzWqNTfMg4mmXDO3kY970MOZh2cco6MdMFTc0zczStOHITj5g\nHfmBcTwOXP6rqq8lefs+v3VPa+2Lh/2gqjqX5FySHD9+/NAFsljTPIh42nBkJx+z0KOH6V8M4QfG\n8TgwVLXWPtTjg1prDyR5IEm2t7dbj2syH5POLE0bju46feJNM1WJnXwM16OH6V8M4QfG8bD8R3fT\nHnNgJx+wjhz9Mh6Ddv9V1U8m+fUkR5N8uaqeaq2d7lIZK2uaJcNr/6wQxbzoYczDkJ7IaqnW5n8n\ne3t7u128eHHunwssTlU90Vpb+bOgxtS/HAMAVxy2f3lMDQB7THtuHIyZmSoA9nAMAExOqAJgD8cA\nwOSEKgD2mOZpCjB2QhUAezgGACZnUB2APRwDAJMTqgDY1xjPjXOMBEMIVQAQx0gwnJkqAIhjJBhO\nqAKAOEaC4YQqAIhjJBhOqAJgKhee3Mnt9z2aW+/+cm6/79FceHJn0SUN4hgJhjKoDsDE1nGo2zES\nDCVUATCxGw11r3IIGeMxEvRj+Q+AiRnqhr3cqQJgYrdsbmRnnwA1z6FuB3WybNypAmBiix7qvjrT\ntfPKpbS8MdO16sPyrDZ3qgCY2LRD3b3uLq3rTBerTagCYCqTDnX33DFopotlZPkPgLno+RgYB3Wy\njIQqAOai592lRc90wX6EKgDmoufdpTOntnL+7MlsbW6kkmxtbuT82ZPmqVgoM1UAzMVdp0+8aaYq\nGXZ3yUGdLBuhCoC58BgY1p1QBcDcuLvEOjNTBQDQgVAFANCBUAUA0IFQBQDQgVAFANCBUAUA0IFQ\nBQDQgVAFANCBUAUA0MGgUFVV91fVd6vq21X1hara7FUYwKzpYUBPQ+9UPZLkttbae5P8UZJPDC8J\nYG70MKCbQaGqtfbV1tpruy8fS/LO4SUBzIceBvTUc6bqZ5P8XsfrAcyTHgYM8paDvqCqvpbk7fv8\n1j2ttS/ufs09SV5L8uANrnMuybkkOX78+FTFAkyqRw/Tv4DDODBUtdY+dKPfr6qfSfKRJD/eWms3\nuM4DSR5Iku3t7et+HUBPPXqY/gUcxoGh6kaq6o4kv5Tk77fWvt+nJID50MOAnobOVP3nJG9L8khV\nPVVV/7VDTQDzoocB3Qy6U9Va+9u9CgGYNz0M6MmJ6gAAHQhVAAAdCFUAAB0IVQAAHQhVAAAdCFUA\nAB0IVQAAHQhVAAAdCFUAAB0IVQAAHQhVAAAdCFUAAB0IVQAAHQhVAAAdCFUAAB0IVQAAHQhVAAAd\nCFUAAB0IVQAAHQhVAAAdCFUAAB0IVQAAHQhVAAAdCFUAAB0IVQAAHQhVAAAdCFUAAB0IVQAAHQhV\nAAAdCFUAAB0IVQAAHQhVAAAdCFUAAB0MClVV9e+r6ttV9VRVfbWqbulVGMCs6WFAT0PvVN3fWntv\na+19SX43yS93qAlgXvQwoJtBoaq19pfXvLw5SRtWDsD86GFAT28ZeoGq+lSSf5Hk/yT5h4MrApgj\nPQzopVq78Q9mVfW1JG/f57fuaa198Zqv+0SSt7bWfuU61zmX5Nzuy9uSfGeqiufrbyX5i0UXcQir\nUmeyOrWqs78TrbW3zftDe/SwFe1fyer8/6HOvlalzmR1aj1U/zowVB1WVR1P8nBr7bZDfO3F1tp2\nlw+eIXX2tyq1qrO/Za/1sD1s2f89rrUqtaqzr1WpM1mdWg9b59Ddf+++5uVHk3x3yPUA5kkPA3oa\nOlN1X1WdSPKDJH+S5F8NLwlgbvQwoJtBoaq19k+n/KMPDPncOVJnf6tSqzr7W7pap+xhS/fvcQOr\nUqs6+1qVOpPVqfVQdXabqQIAGDOPqQEA6GBhoWpVHg9RVfdX1Xd3a/1CVW0uuqb9VNVPVdUzVfWD\nqlq6nRRVdUdVPV9VL1TV3Yuu53qq6jNV9b2qWuot81V1rKq+XlXP7v53//iia9pPVb21qv6gqv5w\nt85fXXRNPehf/elhw+lffU3Tvxa2/FdVf/PqacZV9a+T/GhrbemGRKvqHyd5tLX2WlX9hyRprf2b\nBZe1R1X9nVwZtv2NJL/YWru44JJeV1VHkvxRkg8neSnJN5Pc2Vp7dqGF7aOq/l6SV5P8t8McD7Io\nVfWOJO9orX2rqt6W5IkkZ5bt77SqKsnNrbVXq+qmJN9I8vHW2mMLLm0Q/as/PWw4/auvafrXwu5U\nrcrjIVprX22tvbb78rEk71xkPdfTWnuutfb8ouu4jvcneaG19settf+b5HdyZfv60mmt/X6S/73o\nOg7SWvvz1tq3dv/5r5I8l2RrsVXt1a54dfflTbu/lvJ7fRL6V3962HD6V1/T9K+FzlRV1aeq6sUk\n/yyr8SDTn03ye4suYgVtJXnxmtcvZQm/gVZVVb0ryakkjy+2kv1V1ZGqeirJ95I80lpbyjonpX+N\nih42I+vWv2Yaqqrqa1X1nX1+fTRJWmv3tNaOJXkwyc/NspYhde5+zT1JXtutdWnrZFyq6keSfC7J\nz/9/d0+WRmvtcmvtfblyl+T9VbW0yxLX0r/608O41jr2r8EPVD6gmA8d8ksfTPJwkn2fGzhrB9VZ\nVT+T5CNJfrwt8AyKCf4+l81OkmPXvH7n7nsMsLvG/7kkD7bWPr/oeg7SWnulqr6e5I6swLPz9K/+\n9DCuWtf+tcjdfyvxeIiquiPJLyX5J6217y+6nhX1zSTvrqpbq+qHk/x0ki8tuKaVtjtA+ekkz7XW\nfm3R9VxPVR29uuOsqjZyZdB3Kb/XJ6F/jY4e1tE6969F7v77XJI3PR6itbZ0yb+qXkjyN5L8r923\nHlvSXT4/meTXkxxN8kqSp1prpxdb1Ruq6ieS/KckR5J8prX2qQWXtK+qeijJP8iVJ6f/zyS/0lr7\n9EKL2kdV/d0k/z3J07nyPZQk/7a19vDiqtqrqt6b5Ldy5b/7DyX5bGvt3y22quH0r/70sOH0r76m\n6V9OVAcA6MCJ6gAAHQhVAAAdCFUAAB0IVQAAHQhVAAAdCFUAAB0IVQAAHQhVAAAd/D/s6oYyXGRC\nTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110eb52b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pl.figure(1, figsize=(10,10))\n",
    "ax1=pl.subplot(121, aspect=1)\n",
    "pl.plot(np.cos(t), np.sin(t), 'o' ,linewidth=1)\n",
    "ax1.set_xlim(-3,3)\n",
    "ax1.set_ylim(-3,3)\n",
    "ax2=pl.subplot(122, aspect = 1)\n",
    "pl.plot(res[0,:],res[1,:], 'o', linewidth=1)\n",
    "ax2.set_xlim(-3,3)\n",
    "ax2.set_ylim(-3,3)\n",
    "print(\"The transformed vectors after multiplying by the matrix A that we've constructed from the eigenvectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Eigendecomposition II\n",
    "- constructing matrices from from eigenvalues and -vectors allows to stretch space in desired directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We usually need to *decompose* a matrix into eigenvalues and eigenvectors to analyze its properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Eigendecomposition does not always exist\n",
    "    - complex eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- but: every real, symmetric matrix can be decomposed\n",
    "    - by convention we sort entries of $\\mathbf{\\Lambda}$ in descending order "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "### Eigendecomposition II\n",
    "\n",
    "- eigendecomposition may not be unique!\n",
    "    - if two or more eigenvectors share the same eigenvalue, then every vector lying in their *span* will also be an eigenvector\n",
    "    - eigendecomposition is unique, only if all eigen*values* are unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Eigendecomposition II\n",
    "\n",
    "- We can read a lot of information from the decomposition of $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the sum of the eigenvalues equals the trace of $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the product of the eigenvalues equals the determinant of $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{A}$ is singular iff any eigenvalues are zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A matrix whose eigenvalues are all:\n",
    "    - positive is *positive definite*\n",
    "    - positive or zero is *positive semidefinite*\n",
    "    - (and vice versa for negative eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- positive semidefinite matrices guarantee $\\forall \\mathbf{x}: \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} \\geq 0.$\n",
    "- positive definite additionally guarantee $ \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} = 0 \\implies \\mathbf{x}=\\mathbf{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### an important application in Machine Learning\n",
    "- PCA is the eigendecomposition of a covariance matrix!\n",
    "- see the explanation here: http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- eigenvector with largest eigenvalue points into direction of largest variance in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- next (largest) eigenvector is orthogonal and points into second largest direction\n",
    "- and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "    \n",
    "![example from visiondummy.com](http://www.visiondummy.com/wp-content/uploads/2014/04/eigenvectors_covariance.png)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The covariance matrix corresponds to a linear transformation (rotation and scaling) of white, uncorrelated data (just as the example above with the transformed unit circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- eigendecompositions gives the eigenvector matrix (rotation) and the eigenvalue matrix (scaling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- from those we can read the directions of largest variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- we often want to optimize a quadratic expression $f(\\mathbf{x}) = \\mathbf{x}^\\top\\mathbf{A}\\mathbf{x}$ subject to $\\lVert\\mathbf{x} \\rVert = 1$ (on the unit circle):\n",
    "- the maximum of f within the onstraint region is the maximum eigenvalue and the minimum of f is the minimum eigenvalue (remember the figure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- $ \\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$ says that the eigenvectors keep the same direction when multiplied by A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the sum of the eigenvalues equals the trace of $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the product of the eigenvalues equals the determinant of $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- eigenvalues determine if $\\mathbf{A}$ is positive(negative) (semi) definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular Value Decomposition\n",
    "- SVD provides another way to factorize a matrix, into singular values and singular vectors.\n",
    "- similar to eigendecomposition but more generally applicable:\n",
    "    - every real matrix has a singular value decomposition\n",
    "    - e.g. non-square matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the eigendecomposition was defined as $\\mathbf{A} = \\mathbf{V} diag(\\mathbf{\\lambda})\\mathbf{V}^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the SVD now is: $\\mathbf{A} = \\mathbf{U} \\mathbf{D}\\mathbf{V}^{\\top}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- if $\\mathbf{A}$ is $m \\times n$ then $\\mathbf{U}$ is $m \\times m$, $\\mathbf{D}$ is $m \\times n$ and $\\mathbf{V}$ is $n \\times n$\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $\\mathbf{A} = \\mathbf{U} \\mathbf{D}\\mathbf{V}^{\\top}$\n",
    "- each matrix has a special structure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal, the columns of $\\mathbf{U}$ are the *left-singular vectors* and the columns of $\\mathbf{V}$ are the *right-singular vectors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{D}$ is diagonal, elements along diagonal are the *singular values* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the connection between SVD and eigendecomposition:\n",
    "    - the left-singular vectors are the eigenvectors of $\\mathbf{A}\\mathbf{A}^\\top$\n",
    "    - the right-singular vectors are the eigenvectors of $\\mathbf{A}^\\top\\mathbf{A}$\n",
    "    - the non-zero singular values are the *square roots* of the eigenvalues of $\\mathbf{A}^\\top\\mathbf{A}$ (the same is true for $\\mathbf{A}\\mathbf{A}^\\top$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- SVD factors $\\mathbf{A}$ into  $\\mathbf{U} \\mathbf{D}\\mathbf{V}^{\\top}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{A}$ doen't need to be square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the squared singular values are the nonzero eigenvalues of $\\mathbf{A}^\\top\\mathbf{A}$ and $\\mathbf{A}\\mathbf{A}^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the orthonormal columns of $\\mathbf{U}$ and $\\mathbf{V}$ are the eigenvectors of $\\mathbf{A}\\mathbf{A}^\\top$ and $\\mathbf{A}^\\top\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Moore-Penrose Pseudoinverse\n",
    "- matrix inversion not defined for matrices that are not square:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- if $\\mathbf{A}$ is taller than wide, we might not get a solution (more equations than unkowns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- if $\\mathbf{A}$ is wider than tall, there could be multiple solutions (more unkowns than equations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This is where we need the Moore-Penrose pseudoinverse to get one (principle) solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $ \\mathbf{A}^{+} = \\lim_{\\alpha \\to 0}(\\mathbf{A}^\\top\\mathbf{A} + \\alpha \\mathbf{I})^{-1}\\mathbf{A}^\\top $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If A is wider than tall (*more unkowns than equations*):\n",
    "    - we get the solution with minimal Euclidean norm among all possible solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If A is taller than wide (*more equations than unkowns*) and there is no solution:\n",
    "    - we get the least squares solution $\\mathbf{x}$, i.e. $\\mathbf{A}\\mathbf{x}$ is as close as possible to $\\mathbf{y}$ in terms of Euclidean norm.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- typically computed using the singular value decomposition: $\\mathbf{A}^{+}=\\mathbf{V}\\mathbf{D}^{+}\\mathbf{U}^\\top $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References \n",
    "\n",
    "![Goodfellow, Bengio, Courville - Chapter 2](https://images.gr-assets.com/books/1478212695l/30422361.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Strang - Linear Algebra](https://images-na.ssl-images-amazon.com/images/I/41Mf6xABXtL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A nice (and somewhat long) example to test your understanding of linear algebra: PCA!\n",
    "[the last part of chapter 2 from Goodfellow book](http://www.deeplearningbook.org/contents/linear_algebra.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nbpresent": {
   "slides": {
    "5aa13a9a-b6c4-4d3d-bd2b-32597e1f5122": {
     "id": "5aa13a9a-b6c4-4d3d-bd2b-32597e1f5122",
     "prev": "8cbfcf9b-399f-4300-b779-fd3d4ec2e1db",
     "regions": {
      "e671e58b-27ca-4411-8a6a-0c9dce9fd7ef": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "c9563bae-f44b-4b42-a984-d4c617609614",
        "part": "whole"
       },
       "id": "e671e58b-27ca-4411-8a6a-0c9dce9fd7ef"
      }
     }
    },
    "8cbfcf9b-399f-4300-b779-fd3d4ec2e1db": {
     "id": "8cbfcf9b-399f-4300-b779-fd3d4ec2e1db",
     "prev": null,
     "regions": {
      "2924e5e3-5a38-424d-9535-f168b30b6774": {
       "attrs": {
        "height": 0.991616766467066,
        "width": 0.9956087824351297,
        "x": 0.002195608782435175,
        "y": 0.005965846085606484
       },
       "content": {
        "cell": "db7323b1-1a66-47cb-bce7-833fc5728820",
        "part": "whole"
       },
       "id": "2924e5e3-5a38-424d-9535-f168b30b6774"
      }
     },
     "theme": null
    }
   },
   "themes": {
    "default": "0092f017-0820-4949-8748-2cd350c48e8f",
    "theme": {
     "0092f017-0820-4949-8748-2cd350c48e8f": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "0092f017-0820-4949-8748-2cd350c48e8f",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         34,
         34,
         34
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         42,
         118,
         221
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         34,
         34,
         34
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 5.25
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 4
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3.5
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       },
       "p": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Source Sans Pro",
       "font-size": 6
      }
     },
     "5a7cc1be-340f-48de-b09f-094ab35ac02e": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "5a7cc1be-340f-48de-b09f-094ab35ac02e",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         34,
         34,
         34
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         66,
         175,
         250
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         256,
         256,
         256
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 5.25
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 4
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3.5
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       },
       "p": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Source Sans Pro",
       "font-size": 6
      }
     },
     "877e6922-7940-4f6d-9596-cbb56beef6cf": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "877e6922-7940-4f6d-9596-cbb56beef6cf",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         43,
         43,
         43
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         238,
         238
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         19,
         218,
         236
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         238,
         238,
         238
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
