{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "db7323b1-1a66-47cb-bce7-833fc5728820"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Algebra\n",
    "    \n",
    "  - What and why? \n",
    "  - Scalars, Vectors, Matrices \n",
    "  - Systems of Linear Equations\n",
    "  - The Transpose\n",
    "  - Matrix operations\n",
    "  - Linear Dependence and Span\n",
    "  - Norms\n",
    "  - Special Kinds of Matrices and Vectors\n",
    "  - Eigendecomposition\n",
    "  - Singular Value Decomposition\n",
    "  - The Moore-Penrose Pseudoinverse\n",
    "  - The Trace Operator\n",
    "  - The Determinant\n",
    "  - final note: Tensors\n",
    "  - (Example: PCA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c9563bae-f44b-4b42-a984-d4c617609614"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Algebra\n",
    "\n",
    "## What and Why?\n",
    "- a branch of mathematics that deals with linear systems\n",
    "    - systems of linear equations $a_1 x_1 + a_2 x_2 + ... + a_n x_n = b $\n",
    "    - linear functions $ (x_1, x_2, ..., x_n) \\mapsto a_1 x_1 + a_2 x_2 + ... + a_n x_n$\n",
    "    - their representations through matrices and vector spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- maybe the most important basis for applied mathematics\n",
    "    - Geometry, \n",
    "    - Computer Graphics,\n",
    "    - Engineering,\n",
    "    - Quantum mechanics,\n",
    "    - Fourier series,\n",
    "    - **Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## History\n",
    "- from the study of determinants to solve systems of linear equations\n",
    "- Leibniz 1693\n",
    "- Cramer 1750\n",
    "- Gauss\n",
    "- Grassman 1844\n",
    "- Sylvester 1848, introduced the term _matrix_\n",
    "- Pasha 1882, the first book on _linear algebra_\n",
    "- Peano 1888, _vector spaces_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "040cc7c9-29fb-4bf8-a210-026ba8359fa1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scalars, Vectors, Matrices, and Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scalars\n",
    "\n",
    "- just a single number, e.g. $a \\in \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vectors\n",
    "- an array of numbers, arranged in order\n",
    "- typically written as $\\vec{x}$ or $\\mathbf{x}$\n",
    "- n real elements then $\\mathbf{x} \\in \\mathbb{R} ^n$\n",
    "- $ \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$\n",
    "- needs _one_ index to specify an element\n",
    "- think of vectors as identifying points in n-dimensional space\n",
    "    - each element giving the coordinate along an axis (dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adding Vectors\n",
    "- \"you can't add apples to oranges\" [Strang]\n",
    "- but you can add the vectors, the components stay separate\n",
    "    - $\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}$ and $\\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}$ add to $\\mathbf{v} + \\mathbf{w}= \\begin{bmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\end{bmatrix}$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![add apples to apples and oranges to oranges](img/add-apples-and-oranges.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ef/Vector_addition.png/640px-Vector_addition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Scalar Multiplication\n",
    "- the second important operation for vectors\n",
    "- we multiply each component (again) by a scalar value\n",
    "    - $2\\mathbf{v}=  \\begin{bmatrix} 2 v_1 \\\\  2 v_2 \\end{bmatrix} $ or:  $-\\mathbf{v}=  \\begin{bmatrix} - v_1 \\\\  - v_2 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear Combinations\n",
    "- Linear algebra is built on these two operations (hence the name linear).\n",
    "- we can form *linear combinations* by multiplying vectors with scalars and adding the results:\n",
    "    - $c\\mathbf{v}+d\\mathbf{w}$ is the prototypic linear combination\n",
    "        - $1\\mathbf{v}+1\\mathbf{w}$ is the vector sum\n",
    "        - $1\\mathbf{v}-1\\mathbf{w}$ is the difference\n",
    "        - $0\\mathbf{v}+0\\mathbf{w}$ *zero vector*\n",
    "        - $c\\mathbf{v}+0\\mathbf{w}$ scaled vector in same direction as $\\mathbf{v}$\n",
    "- all possible linear combinations of n vectors typically fill the entire n-dimensional space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/3/3c/Linear_combination_in_2D_space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Representing Vectors\n",
    "- as two numbers (or n numbers in n-dimensional space)\n",
    "- as an arrow from the origin\n",
    "- as a point in the plane (or n-dimensional space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- a vector in n dimensions has n components\n",
    "- $\\mathbf{v}+\\mathbf{w}=(v_1+v_2, w_1+w_2)$ and $c\\mathbf{v}=(c v_1, c v_2)$: one component at a time!\n",
    "- linear combination of three vectors is: $c \\mathbf{u}+ d\\mathbf{v}+e\\mathbf{w}$\n",
    "- all combinations of three (independent) vectors fill the whole $\\mathbb{R}^3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lengths, Angles, and the Dot (inner) Product\n",
    "- in 2D: between vectors $x$ and $y$ of the same dimensionality: $v \\cdot w = v_1 w_1 + v_2 w_2$\n",
    "    - commutative: the order doesn't matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![source: betterexplained.com/](https://betterexplained.com/wp-content/uploads/dotproduct/dot_product_components.png)\n",
    "- source: betterexplained.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- defines length of vectors\n",
    "    - length of $\\mathbf{v}$ is square root of inner product of $\\mathbf{v}$ with itself: $\\lVert \\mathbf{v} \\rVert = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}} $\n",
    "- a *unit vector* has length of 1\n",
    "    - $\\mathbf{u} = \\frac{\\mathbf{v}}{\\lVert \\mathbf{v} \\rVert}$ is the unit vector in the direction of $\\mathbf{v}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [1 1]\n",
      "length of v: 1.41421356237\n"
     ]
    }
   ],
   "source": [
    "v=np.array([1,1])\n",
    "print(\"v: \"+str(v))\n",
    "print(\"length of v: \" + str(np.sqrt(np.dot(v,v))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: [1 1]\n",
      "length of v: 1.41421356237\n",
      "unit vector in direction of v: [ 0.70710678  0.70710678]\n",
      "length of unit vector: 1.0\n"
     ]
    }
   ],
   "source": [
    "v=np.array([1,1])\n",
    "norm_v = np.sqrt(np.dot(v,v))\n",
    "u = v / norm_v \n",
    "print(\"v: \"+str(v))\n",
    "print(\"length of v: \"+str(norm_v))\n",
    "print(\"unit vector in direction of v: \"+str(u))\n",
    "print(\"length of unit vector: \"+str(np.sqrt(np.dot(u,u))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- defines the angle between vectors\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Dot_Product.svg/220px-Dot_Product.svg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the dot product is 0 when v is perpendicular to w!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.array([1,0]),np.array([0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- more generally, if $\\mathbf{v}$ and $\\mathbf{w}$ are nonzero vectors: \n",
    "    - $\\frac{\\mathbf{v}\\cdot \\mathbf{w}}{\\lVert \\mathbf{v} \\rVert \\lVert \\mathbf{w} \\rVert} = \\cos \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- instead of $\\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} \\cdot \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}$ we often write $\\begin{bmatrix} v_1 & v_2 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}$, i.e. $\\mathbf{v}^\\top \\mathbf{w}$, hence the name: *inner product*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- dot (inner) product multiplies each component and adds the results\n",
    "- length of a vector is the square root of the dot product with itself\n",
    "- a vector divided by its length gives the unit vector in the same direction\n",
    "- the dot product is 0 when the vectors are perpendicular\n",
    "- the cosine of the angle between vectors is the dot product normalized by the multiplied lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices\n",
    "\n",
    "- a 2D array of numbers\n",
    "- upper-case variables $\\mathbf{A}$\n",
    "- real-valued, $m$ rows and $n$ columns, then $\\mathbf{A} \\in \\mathbb{R}^{m \\times n} $\n",
    "- a specific element $A_{i,j}$ \n",
    "    - needs _two indices_ to identify member\n",
    "- $ \\mathbf{A} = \\begin{bmatrix} a_{1,1} & a_{1,2}  \\\\ a_{2,1} & a_{2,2} \\\\ a_{3,1} & a_{3,2} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiplying Matrices and Vectors\n",
    "- $\\mathbf{y}=\\mathbf{A}\\mathbf{x}$, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$, then $\\mathbf{y} \\in \\mathbb{R}^{m \\times 1}$:\n",
    " - $y_{i,1}=\\sum_{k} A_{i,k} x_{k,1}$\n",
    "- y is a linear combination of A's columns (weights defined by x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From Vectors to Matrices\n",
    "- $u = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0\\end{bmatrix}, v = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1\\end{bmatrix}, w = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1\\end{bmatrix}$ \n",
    "- the combination $c\\mathbf{u} + d\\mathbf{v} + e\\mathbf{w} = \\begin{bmatrix}c \\\\d-c \\\\ e-d \\end{bmatrix}$\n",
    "- let's rewrite (stack) the vectors into matrix and multiply with the vector of the coefficients: \n",
    "    - $ \\begin{bmatrix}1 & 0 & 0  \\\\ -1 & 1 & 0 \\\\ 0 & -1 & 1 \\end{bmatrix} \\begin{bmatrix}c \\\\d \\\\ e \\end{bmatrix} = \\begin{bmatrix}c \\\\d-c \\\\ e-d \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### From Vectors to Matrices II\n",
    "- $\\mathbf{y}=\\mathbf{A}\\mathbf{x}$, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$, then $\\mathbf{y} \\in \\mathbb{R}^{m \\times 1}$:\n",
    "- *rows times column (vector)*: $y_{i}=\\sum_{k} A_{i,k} x_{k}$\n",
    "\n",
    "\n",
    "- the matrix A acts on the vector x\n",
    "    - the result b is *a linear combination of the columns* of A: Ax = b\n",
    "    - keep this in mind\n",
    "    - this is a linear system of equations! We will get back to this..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\begin{bmatrix} 1 & 2  \\\\ 3 & 4 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5]\n",
      " [11]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(np.array([[1,2],[3,4]]),np.array([[1], [2]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## System of Linear Equations\n",
    "\n",
    "- $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$\n",
    "    - $\\mathbf{A}\\in\\mathbb{R}^{m \\times n}$ the coefficients, $\\mathbf{b}\\in\\mathbb{R}^{m}$ a known vector, and $\\mathbf{x}\\in\\mathbb{R}^{n}$ a vector of unknown variables\n",
    "- each row of $\\mathbf{A}$ and entry of $\\mathbf{b}$ provide a constraint, e.g. $\\mathbf{A}_{1,1}x_1+\\mathbf{A}_{1,2}x_2+...+\\mathbf{A}_{1,n}x_n = b_1$\n",
    "    - basically each row defines a (hyper)plane in n-dimensional space \n",
    "    - we get a solution if all of them intersect in the same place\n",
    "- How to solve such equations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identity Matrix\n",
    "\n",
    "- $\\mathbf{I}_n\\mathbf{x}=\\mathbf{x}$, $\\forall \\mathbf{x}\\in\\mathbb{R}^n$ \n",
    "- so in 2D: $\\mathbf{I}_2=\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Inverse matrix\n",
    "\n",
    "- $\\mathbf{A}^{-1}$ is the matrix inverse of $\\mathbf{A}$, defined as:\n",
    "- $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### solving equations  I\n",
    "\n",
    "- $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{I}_{n}\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$\n",
    "\n",
    "- several methods for finding inverse if it exists\n",
    "- in practice the inverse is usually not computed directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- Matrix times vector: Ax = combination of columns of A\n",
    "- The solution to $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ is $\\mathbf{x} = \\mathbf{A}^{-1}\\mathbf{b}$, when $\\mathbf{A}$ is invertible.\n",
    "- The inverse matrix gives $\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}_n$ and $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}_n$\n",
    "- if $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$ for a nonzero vector x then A has no inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## the Transpose\n",
    "\n",
    "- important and much simpler as inverse\n",
    "- mirror image of the matrix across main diagonal: $(\\mathbf{A}^\\top)_{i,j}=A_{j,i}$\n",
    "    - the columns become the rows\n",
    "- vectors can be seen as 1D matrices, i.e:  $\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$ can be written as $\\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix}^\\top$ \n",
    "- a scalar is a 1x1 matrix, i.e. $a^\\top = a $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2,3],[4,5,6]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "print(np.transpose(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $\\mathbf{A}\\mathbf{x}$ combines the columns of A while $\\mathbf{x^\\top}\\mathbf{A^\\top}$ combines the rows of $\\mathbf{A^\\top}$: \n",
    "    - the same combination of the same vectors!\n",
    "- more mathematically speaking: $\\mathbf{A^\\top}$ is the matrix that makes those two products equal for every x and y:\n",
    "    - $(\\mathbf{A}\\mathbf{x})^\\top \\mathbf{y} = \\mathbf{x}^\\top (\\mathbf{A}^\\top \\mathbf{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- that explains the name \"inner products\":\n",
    "    - $^\\top$ inside: the dot or inner product: $\\mathbf{x}^\\top \\mathbf{y}, (1 \\times n)(n \\times 1)$\n",
    "    - $^\\top$ outside: the rank or outer product: $\\mathbf{x}\\mathbf{y}^\\top, (n \\times 1)(1 \\times n)$\n",
    "- Inner products are everywhere...\n",
    "    - work = movements forces $= \\mathbf{x}^\\top \\mathbf{f}$\n",
    "    - income = quantities prices $= \\mathbf{q}^\\top \\mathbf{p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- the transpose puts rows of $\\mathbf{A}$ into the columns of $\\mathbf{A}^\\top$\n",
    "- the dot product is $\\mathbf{x}^\\top \\mathbf{y}$\n",
    "    - thus, $(\\mathbf{A} \\mathbf{x})^\\top \\mathbf{y} = \\mathbf{x}^\\top (\\mathbf{A}^\\top \\mathbf{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## (some) Matrix computations\n",
    "### Matrix addition \n",
    "\n",
    "- $\\mathbf{C}=\\mathbf{A}+\\mathbf{B}$, where $\\mathbf{C}_{i,j}= \\mathbf{A}_{i,j}+\\mathbf{B}_{i,j}$  \n",
    "\n",
    "### scalar multiplication\n",
    "\n",
    "- $\\mathbf{D} = a \\mathbf{B}$, where $\\mathbf{D}_{i,j} = a \\mathbf{B}_{i,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiplying matrices\n",
    "- $\\mathbf{C}=\\mathbf{A}\\mathbf{B}$, defined only if $\\mathbf{A}$ has as many columns as $\\mathbf{B}$ has rows\n",
    "- $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{B} \\in \\mathbb{R}^{n \\times p}$, then $\\mathbf{C} \\in \\mathbb{R}^{m \\times p}$:\n",
    " - $C_{i,j}=\\sum_{k} A_{i,k} B_{k,j}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multiplying Matrices and Vectors\n",
    "- $\\mathbf{y}=\\mathbf{A}\\mathbf{x}$, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{x} \\in \\mathbb{R}^{n \\times 1}$, then $\\mathbf{y} \\in \\mathbb{R}^{m \\times 1}$:\n",
    " - $y_{i,1}=\\sum_{k} A_{i,k} x_{k,1}$\n",
    "- y is a linear combination of A's columns (weights defined by x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### some useful Properties\n",
    "\n",
    "- distributive: $\\mathbf{A}\\left(\\mathbf{B}+\\mathbf{C}\\right) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}$\n",
    "- associative: $\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C} $\n",
    "- *not* commutative: $\\mathbf{A}\\mathbf{B}$ not always the same as $\\mathbf{B}\\mathbf{A} $ \n",
    "    - but the inner vector product is: $\\mathbf{x}^\\top \\mathbf{y} = \\mathbf{y}^\\top \\mathbf{x}$\n",
    "- transpose of product: $(\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top\\mathbf{A}^\\top$\n",
    "\n",
    "- a little proof: \n",
    "    - $\\mathbf{x}^\\top \\mathbf{y} = (\\mathbf{x}^\\top \\mathbf{y})^\\top $, since it is a scalar.\n",
    "    - $(\\mathbf{x}^\\top \\mathbf{y})^\\top  = \\mathbf{y}^\\top \\mathbf{x}$\n",
    "- many more properties exist. Consult a textbook..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Dependence and Span\n",
    "### Solving Equations (again)\n",
    "- $\\mathbf{A}^{-1}$ only exists if $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ has exactly one solution for each b\n",
    "    - all the hyperplanes need to intersect in a common point\n",
    "    - but it could have no solution or infinitely many solutions.\n",
    "    - it cannot have more than 1 but less than infinitely many: $\\alpha\\mathbf{x}+(1-\\alpha)\\mathbf{y}$\n",
    "- columns of A define vectors (directions)\n",
    "- Can we reach $\\mathbf{b}$ by taking steps of certain size along these directions? In how many ways? $\\mathbf{x}$ specifies these step lengths: $\\mathbf{A}\\mathbf{x} = \\sum_{i}x_i \\mathbf{A}_{:,i}$\n",
    "- this operation is a *linear combination*:\n",
    "    - reminder: a linear combination of a set of vectors $\\{\\mathbf{v}^{(1)},\\mathbf{v}^{(1)},...,\\mathbf{v}^{(n)}\\}$ is given by multiplying each $\\mathbf{v}^{(i)}$ by a scalar and sum over results: $\\sum_{i} c_i \\mathbf{v}^{(i)} $.\n",
    "- the *span* of a set of vectors is the set of all points obtainable by linear combinations of those vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solving Equations II\n",
    "- $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ has a solution if $\\mathbf{b}$ is in the span of the columns of $\\mathbf{A}$.\n",
    "    - this is called the *column space* of $\\mathbf{A}$ (or the range).\n",
    "- to have solutions for all $\\mathbf{b} \\in \\mathbb{R}^m $ the column space of $\\mathbf{A}$ needs to be all of  $\\mathbb{R}^m $.\n",
    "    - i.e. $\\mathbf{A}$ needs at least $m$ columns.\n",
    "    - e.g. a $3 \\times 2$ matrix: $\\mathbf{b}$ is 3D but $\\mathbf{x}$ is only 2D. The column space has only 2 basis vectors, it can only trace out a 2-D plane. There are only solutions for $\\mathbf{b}$ on that plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear Dependence\n",
    "- $n \\geq m$ (more columns than rows) is only necessary but not sufficient as columns could be redundant.\n",
    "    - $\\begin{bmatrix} 0 & 0 \\\\ 1 & 1 \\end{bmatrix}$ has two columns, but column space is a single line.\n",
    "    - this is called *linear dependence*.\n",
    "- A set of vectors is linear *independent* if no vector (in the set) is a linear combination of the other vectors. \n",
    "- Thus, to have a column space to encompass $\\mathbb{R}^{m}$, the matrix must have at least $m$ linearly independent columns.\n",
    "- $\\mathbf{A}$ is invertible if it has at most one solution for each value of $\\mathbf{b}$, i.e. it has at most $m$ columns.\n",
    "    - i.e. the matrix must be square ($m=n$) and all columns must be linearly independent. Otherwise it is *singular*.\n",
    "    - it is still possible to solve the equation if $\\mathbf{A}$ is singular or non-square but not with matrix inversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Least Squares Solutions and the Transpose\n",
    "\n",
    "- When $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ has no solution, multiply by $\\mathbf{A}^\\top$ and solve $\\mathbf{A}^\\top \\mathbf{A}\\mathbf{x} =  \\mathbf{A}^\\top \\mathbf{b}$\n",
    "- Why?\n",
    "    - short answer: \n",
    "        - the columns of $\\mathbf{A}$ span only a small part of the m-dimensional space \n",
    "        - All the potential $\\mathbf{b}$s that would admit a solution to $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ lie on a subspace (hyperplane)\n",
    "        - For all others $\\mathbf{b}$s we will have a remaining error term $\\mathbf{e} = \\mathbf{b}- \\mathbf{A}\\mathbf{x}$\n",
    "        - We want to look for the point closest to $\\mathbf{b}$ in this plane\n",
    "        - that point comes from $\\mathbf{A}^\\top(\\mathbf{b}-\\mathbf{A}\\mathbf{x})=\\mathbf{0}$\n",
    "            - the error $(\\mathbf{b}-\\mathbf{A}\\mathbf{x})$ makes a right angle with all (column) vectors $\\mathbf{a}_i$ of $\\mathbf{A}$:\n",
    "            - i.e. the inner product $\\mathbf{a}_i^\\top(\\mathbf{b}-\\mathbf{A}\\mathbf{x})$ is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- $\\mathbb{R}^n$ contains all column vectors with n real components\n",
    "- a set of vectors is independent if no vector is a linear combination of the others\n",
    "    - the columns of a matrix are independent if $\\mathbf{x}=\\mathbf{0}$ is the only solution to $\\mathbf{A}\\mathbf{x}=\\mathbf{0}$\n",
    "- a set of vectors *span* a space if their combinations fill that space\n",
    "    - the combinations of columns of A form the *column space* \n",
    "        - the column space is *spanned* by the columns\n",
    "- $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ has a solution exactly when $\\mathbf{b}$ is in the column space of $\\mathbf{A}$\n",
    "    - otherwise we can obtain a least squares solution by using the transpose of A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Norms\n",
    "- we often need to measure the size of a vector, we usually use a function called a *norm* ($L^p$ norm): \n",
    "    - $||\\mathbf{x}||_{p} = \\left( \\sum_{i} \\left|x_i\\right|^p \\right) ^ {\\frac{1}{p}}$\n",
    "- norms map vectors to non-negative values.\n",
    "- measure \"distance to the origin\"\n",
    "- formally, a norm is any function $f$ satisfying:\n",
    "    - $f(\\mathbf{x}=0) \\implies \\mathbf{x}=\\mathbf{0}$\n",
    "    - $f(x+y)\\leq f(x) + f(y)$\n",
    "    - $\\forall \\alpha \\in \\mathbb{R}, f(\\alpha \\mathbf{x}) = \\left|\\alpha \\right|f(\\mathbf{x})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The $L^2$ norm is the \"standard\" Euclidean norm and is often denoted by $\\lVert \\mathbf{x} \\rVert$. It gives the Euclidean distance from the origin.\n",
    "- We can also measure the size of $\\mathbf{x}$ using the *squared* $L^2$ norm: $\\mathbf{x}^\\top \\mathbf{x}$.\n",
    "    - easier to work with mathematically and computationally:\n",
    "    - its derivatives with respect to each element depend only on that element, while for $L^2$ it depends on the entire vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In an optimization context, squared $L^2$ is not so desirable, because it increases slowly near 0. We often want to discriminate between *near zero* and *zero*. For this the $L^1$ norm is used: $$ \\lVert \\mathbf{x} \\rVert _{1} = \\sum_i \\left| x_i \\right|$$\n",
    "- Sometimes, the size of a vector is measured by counting the number of nonzero elements. This is not a norm (scaling property).\n",
    "- Another common form is the $L^\\infty$ or *max norm*: $$ \\lVert \\mathbf{x} \\rVert _{\\infty} = \\max_i \\left| x_i \\right| $$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To measure the size of a matrix, the most common is the *Frobenius norm* $$\\lVert \\mathbf{A} \\rVert _{F} = \\sqrt{\\sum_{i,j} A^2_{i,j} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Special Kinds of Matrices and Vectors\n",
    "### Diagonal Matrices\n",
    "- $D_{i,j}=0$,  $\\forall i\\neq j$\n",
    "    - Identity matrix\n",
    "    - $diag(\\mathbf{v})$\n",
    "    - computationally very efficient: \n",
    "        - $diag(\\mathbf{v})\\mathbf{x} = \\mathbf{v}\\circ \\mathbf{x}$\n",
    "        - $diag(\\mathbf{v})^{-1} = diag([\\frac{1}{v_1},...,\\frac{1}{v_n}]^\\top)$\n",
    "    - we often derive machine learning methods in terms of arbitrary matrices, but obtain less expensive (and less descriptive) approximations by restricting to diagonal matrices.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Orthogonal Matrices\n",
    "- a *unit vector*: $\\lVert \\mathbf{x} \\rVert = 1$\n",
    "- $\\mathbf{x}$ and $\\mathbf{y}$ are *orthogonal* if $\\mathbf{x}^\\top \\mathbf{y} = 0$\n",
    "    - 90 degree angle between them\n",
    "    - in $\\mathbb{R}^n$ at most $n$ vectors can be orthogonal.\n",
    "    - if vectors are orthogonal and unit, they are called orthonormal\n",
    "- An *orthogonal matrix* is a square matrix with mutually orthonormal columns (and rows): \n",
    "    - $ \\mathbf{A}^\\top \\mathbf{A} = \\mathbf{A} \\mathbf{A}^\\top = \\mathbf{I}$\n",
    "    - $\\implies \\mathbf{A}^{-1} = \\mathbf{A}^\\top$\n",
    "    - inverse is very cheap to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Eigendecomposition\n",
    "- mathematical objects can be understood better by finding universal properties independent of representation\n",
    "    - integer decomposition into prime factors: \n",
    "        - 12 vs 1100\n",
    "        - but 12 = 2 x 2 x 3 will always be true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- we can discover functional properties of matrices in a similar way: *Eigendecomposition*\n",
    "    - Matrix -> set of eigenvectors and eigenvalues\n",
    "    - *eigenvector* of $\\mathbf{A}$ is a (non-zero) vector $\\mathbf{v}$ such that multiplication with $\\mathbf{A}$ only changes the length of $\\mathbf{v}$: \n",
    "        - $ \\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\lambda$ (a scalar) is the *eigenvalue* corresponding to the eigenvector $\\mathbf{v}$\n",
    "- we usually only consider *unit* eigenvectors ($\\alpha \\mathbf{v}$ will also be an eigenvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- If $\\mathbf{A}$ has n linearly independent eigenvectors, we can concatenate them into a matrix $\\mathbf{V}$ with one eigenvector per column: $\\mathbf{V} = \\begin{bmatrix} \\mathbf{v}^1 & ... & \\mathbf{v}^n \\end{bmatrix}$ and the eigenvalues into a vector $\\mathbf{\\lambda} = \\begin{bmatrix} \\mathbf{\\lambda}^1 & ... & \\mathbf{\\lambda}^\\top \\end{bmatrix}$\n",
    "    - the eigendecomposition is then given by $$\\mathbf{A} = \\mathbf{V} diag(\\mathbf{\\lambda})\\mathbf{V}^{-1}$$\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116896208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.linspace(0,np.pi*2,100)\n",
    "pl.plot(np.cos(t), np.sin(t), linewidth=1)\n",
    "pl.axes().set_xlim(-3,3)\n",
    "pl.axes().set_ylim(-3,3)\n",
    "pl.axes().set_aspect('equal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's construct a specfic matrix from eigenvectors and eigenvalues and see how it acts on vectors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st eigenvector:  [ 0.70710678  0.70710678] with eigenvalue:  2\n",
      "2nd eigenvector:  [ 0.70710678 -0.70710678] with eigenvalue:  1\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array((1,1))/np.sqrt(2)\n",
    "v2 = np.array((1,-1))/np.sqrt(2)\n",
    "lambda1 = 2\n",
    "lambda2 = 1\n",
    "print(\"1st eigenvector: \", v1, \"with eigenvalue: \", lambda1)\n",
    "print(\"2nd eigenvector: \", v2, \"with eigenvalue: \", lambda2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the matrix V composed of the eigenvectors: \n",
      " [[ 0.70710678  0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "V=np.column_stack((v1,v2))\n",
    "print(\"the matrix V composed of the eigenvectors: \\n\",V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the diagonal matrix containing the eigenvalues: \n",
      " [[2 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "diagLambda=np.diag(np.array((lambda1,lambda2)))\n",
    "print(\"the diagonal matrix containing the eigenvalues: \\n\",diagLambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing V diag(Lambda) V^-1 \n",
      " [[ 1.5  0.5]\n",
      " [ 0.5  1.5]]\n"
     ]
    }
   ],
   "source": [
    "A=np.dot(np.dot(V,diagLambda), np.linalg.inv(V))\n",
    "print(\"computing V diag(Lambda) V^-1 \\n\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the eigendecomposition to check: \n",
      "\n",
      "the eigenvalues\n",
      "[ 2.  1.]\n",
      "the eigenvectors\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing the eigendecomposition to check: \\n\")\n",
    "evals, evecs = np.linalg.eig(A)\n",
    "print(\"the eigenvalues\")\n",
    "print(evals)\n",
    "print(\"the eigenvectors\")\n",
    "print(evecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x=[np.array((np.cos(p), np.sin(p))) for p in np.linspace(0,np.pi*2,100)]\n",
    "res=np.dot(A,np.transpose(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original vectors on the unit circle\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1169d9860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The original vectors on the unit circle\")\n",
    "t = np.linspace(0,np.pi*2,100)\n",
    "pl.plot(np.cos(t), np.sin(t), linewidth=1)\n",
    "pl.axes().set_xlim(-3,3)\n",
    "pl.axes().set_ylim(-3,3)\n",
    "pl.axes().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed vectors after multiplying by the matrix we constructed from the eigenvectors\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njPG4/wcc0/slbOUDsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116a97a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The transformed vectors after multiplying by the matrix we constructed from the eigenvectors\")\n",
    "pl.plot(res[0,:],res[1,:], linewidth=1)\n",
    "pl.axes().set_xlim(-3,3)\n",
    "pl.axes().set_ylim(-3,3)\n",
    "pl.axes().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Eigendecomposition II\n",
    "- constructing matrices from from eigenvalues and -vectors allows to stretch space in desired directions.\n",
    "- We usually need to *decompose* a matrix into eigenvalues and eigenvectors to analyze its properties.\n",
    "- Eigendecomposition does not always exist\n",
    "    - complex eigenvalues and eigenvectors\n",
    "- but: every real, symmetric matrix can be decomposed into this form: $$\\mathbf{A} = \\mathbf{Q}\\mathbf{\\Lambda} \\mathbf{Q}^\\top$$\n",
    "    - $\\mathbf{Q}$ is orthogonal composed of eigenvectors from A\n",
    "    - $\\mathbf{\\Lambda}$ is diagonal \n",
    "    - as we have seen in our construction example...\n",
    "    - by convention we sort entries of $\\mathbf{\\Lambda}$ in descending order \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- eigendecomposition may not be unique!\n",
    "    - if two or more eigenvectors share the same eigenvalue, then every vector lying in their *span* will also be an eigenvector\n",
    "    - eigendecomposition is unique, only if all eigen*values* are unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can read a lot of information from the decomposition of $\\mathbf{A}$\n",
    "    - the sum of the eigenvalues equals the trace of $\\mathbf{A}$\n",
    "    - the product of the eigenvalues equals the determinant of $\\mathbf{A}$\n",
    "    - $\\mathbf{A}$ is singular iff any eigenvalues are zero\n",
    "    - A matrix whose eigenvalues are all:\n",
    "        - positive is *positive definite*\n",
    "        - positive or zero is *positive semidefinite*\n",
    "        - (and vice versa for negative eigenvalues)\n",
    "        - positive semidefinite matrices guarantee $\\forall \\mathbf{x}: \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} \\geq 0.$\n",
    "        - positive definite additionally guarantee $ \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} = 0 \\implies \\mathbf{x}=\\mathbf{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- we often want to optimize a quadratic expression $f(\\mathbf{x}) = \\mathbf{x}^\\top\\mathbf{A}\\mathbf{x}$ subject to $\\lVert\\mathbf{x} \\rVert = 1$ (on the unit circle):\n",
    "- the maximum of f within the onstraint region is the maximum eigenvalue and the minimum of f is the minimum eigenvalue (remember the figure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- $ \\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v}$ says that the eigenvectors keep the same direction when multiplied by A\n",
    "- the sum of the eigenvalues equals the trace of $\\mathbf{A}$\n",
    "- the product of the eigenvalues equals the determinant of $\\mathbf{A}$\n",
    "- eigenvalues determine if $\\mathbf{A}$ is positive(negative) (semi) definite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Singular Value Decomposition\n",
    "- SVD provides another way to factorize a matrix, into singular values and singular vectors.\n",
    "    - similar to eigendecomposition but more generally applicable:\n",
    "        - every real matrix has a singular value decomposition\n",
    "        - e.g. non-square matrices\n",
    "    - the eigendecomposition was defined as $$\\mathbf{A} = \\mathbf{V} diag(\\mathbf{\\lambda})\\mathbf{V}^{-1}$$\n",
    "    - the SVD now is: $$\\mathbf{A} = \\mathbf{U} \\mathbf{D}\\mathbf{V}^{\\top}$$\n",
    "        - if $\\mathbf{A}$ is $m \\times n$ then $\\mathbf{U}$ is $m \\times m$, $\\mathbf{D}$ is $m \\times n$ and $\\mathbf{V}$ is $n \\times n$\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "   $$\\mathbf{A} = \\mathbf{U} \\mathbf{D}\\mathbf{V}^{\\top}$$\n",
    "       \n",
    "- each matrix has a special structure: \n",
    "    - $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal, the columns of $\\mathbf{U}$ are the *left-singular vectors* and the columns of $\\mathbf{V}$ are the *right-singular vectors*\n",
    "    - $\\mathbf{D}$ is diagonal, elements along diagonal are the *singular values* \n",
    "- the connection between SVD and eigendecomposition:\n",
    "    - the left-singular vectors are the eigenvectors of $\\mathbf{A}\\mathbf{A}^\\top$\n",
    "    - the right-singular vectors are the eigenvectors of $\\mathbf{A}^\\top\\mathbf{A}$\n",
    "    - the non-zero singular values are the *square roots* of the eigenvalues of $\\mathbf{A}^\\top\\mathbf{A}$ (the same is true for $\\mathbf{A}\\mathbf{A}^\\top$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Ideas\n",
    "- SVD factors $\\mathbf{A}$ into  $\\mathbf{U} \\mathbf{D}\\mathbf{V}^{\\top}$\n",
    "- the squared singular values are the nonzero eigenvalues of $\\mathbf{A}^\\top\\mathbf{A}$ and $\\mathbf{A}\\mathbf{A}^\\top$\n",
    "- the orthonormal columns of $\\mathbf{U}$ and $\\mathbf{V}$ are the eigenvectors of $\\mathbf{A}\\mathbf{A}^\\top$ and $\\mathbf{A}^\\top\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Moore-Penrose Pseudoinverse\n",
    "- inversion not defined for matrices that are not square\n",
    "- We need a (left-)inverse $\\mathbf{B}$ of $\\mathbf{A}$ to solve $$\\mathbf{A}\\mathbf{x}=\\mathbf{y}$$ by left-multiplying to get: $$\\mathbf{x}= \\mathbf{B}\\mathbf{y}$$\n",
    "    - this may not always work!\n",
    "        - if $\\mathbf{A}$ is taller than wide, we might not get a solution (more equations than unkowns)\n",
    "        - if $\\mathbf{A}$ is wider than tall, there could be multiple solutions (more unkowns than equations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- This is where we need the Moore-Penrose pseudoinverse to get one (principle) solution: \n",
    "    - $ \\mathbf{A}^{+} = \\lim_{\\alpha \\to 0}(\\mathbf{A}^\\top\\mathbf{A} + \\alpha \\mathbf{I})^{-1}\\mathbf{A}^\\top $\n",
    "    - typically computed using the singular value decomposition: $\\mathbf{A}^{+}=\\mathbf{V}\\mathbf{D}^{+}\\mathbf{U}^\\top $\n",
    "- If A is wider than tall (*more unkowns than equations*):\n",
    "    - we get the solution with minimal Euclidean norm among all possible solutions.\n",
    "- If A is taller than wide (*more equations than unkowns*) and there is no solution:\n",
    "    - we get the least squares solution $\\mathbf{x}$, i.e. $\\mathbf{A}\\mathbf{x}$ is as close as possible to $\\mathbf{y}$ in terms of Euclidean norm.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Trace Operator\n",
    "- gives the sum of the diagonal elements: $$Tr(\\mathbf{A})=\\sum_{i}A_{i,i} $$\n",
    "    - remember: it is the sum of the eigenvalues\n",
    "    - some operations are difficult to specify without summation notation can be written in matrix products and trace operations:\n",
    "        - e.g. $\\lVert \\mathbf{A} \\rVert = \\sqrt{Tr(\\mathbf{A}\\mathbf{A}^\\top)} $\n",
    "        - trace is invariant to transposition $Tr(\\mathbf{A})==Tr(\\mathbf{A}^\\top)$\n",
    "        - $Tr(\\mathbf{A}\\mathbf{B}\\mathbf{C})=Tr(\\mathbf{C}\\mathbf{A}\\mathbf{B})=Tr(\\mathbf{B}\\mathbf{C}\\mathbf{A})$\n",
    "            - invariant to cyclic permutations\n",
    "        - a scalar is its own trace $a = Tr(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Determinant\n",
    "- the determinant of a square matrix $\\det(A)$ is a function (from matrices to real scalars)\n",
    "    - remember: it is equal to the product of eigenvalues\n",
    "- absolute value of determinant measures how multiplication with $\\mathbf{A}$ expands or contracts space\n",
    "    - if it is 1 , the transformation is volume preserving\n",
    "        - e.g. deformation fields in registrations (determinant of the Jacobian)\n",
    "    - if it is 0, all space is contracted (at least in one dimension)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## a final note: Tensors\n",
    "- a 'generalization' of scalars, vectors, matrices: \n",
    "    - an array of numbers arranged in a grid with variable number of axis \n",
    "- e.g. $A_{i,j,k}$ \n",
    "    - would need _three_ indices per entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References \n",
    "\n",
    "![Goodfellow, Bengio, Courville - Chapter 2](https://images.gr-assets.com/books/1478212695l/30422361.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Strang - Linear Algebra](https://images-na.ssl-images-amazon.com/images/I/41Mf6xABXtL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A nice (and somewhat long) example to test your understanding of linear algebra: PCA!\n",
    "[the last part of chapter 2 from Goodfellow book](http://www.deeplearningbook.org/contents/linear_algebra.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nbpresent": {
   "slides": {
    "5aa13a9a-b6c4-4d3d-bd2b-32597e1f5122": {
     "id": "5aa13a9a-b6c4-4d3d-bd2b-32597e1f5122",
     "prev": "8cbfcf9b-399f-4300-b779-fd3d4ec2e1db",
     "regions": {
      "e671e58b-27ca-4411-8a6a-0c9dce9fd7ef": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "c9563bae-f44b-4b42-a984-d4c617609614",
        "part": "whole"
       },
       "id": "e671e58b-27ca-4411-8a6a-0c9dce9fd7ef"
      }
     }
    },
    "8cbfcf9b-399f-4300-b779-fd3d4ec2e1db": {
     "id": "8cbfcf9b-399f-4300-b779-fd3d4ec2e1db",
     "prev": null,
     "regions": {
      "2924e5e3-5a38-424d-9535-f168b30b6774": {
       "attrs": {
        "height": 0.991616766467066,
        "width": 0.9956087824351297,
        "x": 0.002195608782435175,
        "y": 0.005965846085606484
       },
       "content": {
        "cell": "db7323b1-1a66-47cb-bce7-833fc5728820",
        "part": "whole"
       },
       "id": "2924e5e3-5a38-424d-9535-f168b30b6774"
      }
     },
     "theme": null
    }
   },
   "themes": {
    "default": "0092f017-0820-4949-8748-2cd350c48e8f",
    "theme": {
     "0092f017-0820-4949-8748-2cd350c48e8f": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "0092f017-0820-4949-8748-2cd350c48e8f",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         34,
         34,
         34
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         42,
         118,
         221
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         34,
         34,
         34
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 5.25
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 4
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3.5
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       },
       "p": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Source Sans Pro",
       "font-size": 6
      }
     },
     "5a7cc1be-340f-48de-b09f-094ab35ac02e": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "5a7cc1be-340f-48de-b09f-094ab35ac02e",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         34,
         34,
         34
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         256,
         256,
         256
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         66,
         175,
         250
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         256,
         256,
         256
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 5.25
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 4
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3.5
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Source Sans Pro",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Source Sans Pro"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       },
       "p": {
        "color": "mainColor",
        "font-family": "Source Sans Pro",
        "font-size": 6
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Source Sans Pro",
       "font-size": 6
      }
     },
     "877e6922-7940-4f6d-9596-cbb56beef6cf": {
      "backgrounds": {
       "backgroundColor": {
        "background-color": "backgroundColor",
        "id": "backgroundColor"
       }
      },
      "id": "877e6922-7940-4f6d-9596-cbb56beef6cf",
      "palette": {
       "backgroundColor": {
        "id": "backgroundColor",
        "rgb": [
         43,
         43,
         43
        ]
       },
       "headingColor": {
        "id": "headingColor",
        "rgb": [
         238,
         238,
         238
        ]
       },
       "linkColor": {
        "id": "linkColor",
        "rgb": [
         19,
         218,
         236
        ]
       },
       "mainColor": {
        "id": "mainColor",
        "rgb": [
         238,
         238,
         238
        ]
       }
      },
      "rules": {
       "a": {
        "color": "linkColor"
       },
       "h1": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 7
       },
       "h2": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 5
       },
       "h3": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3.75
       },
       "h4": {
        "color": "headingColor",
        "font-family": "Oswald",
        "font-size": 3
       },
       "h5": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h6": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "h7": {
        "color": "headingColor",
        "font-family": "Oswald"
       },
       "li": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       },
       "p": {
        "color": "mainColor",
        "font-family": "Lato",
        "font-size": 5
       }
      },
      "text-base": {
       "color": "mainColor",
       "font-family": "Lato",
       "font-size": 5
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
