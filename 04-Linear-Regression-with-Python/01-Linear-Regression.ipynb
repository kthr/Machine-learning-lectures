{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this session, we've modified notebooks from several sources:\n",
    "\n",
    "- [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
    "*The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!*\n",
    "\n",
    "- [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IPython/Jupyter notebook \n",
    "\n",
    "The notebook interface is designed to be an interactive computing environment, where you can mix code, data, documentation and outputs. This a very good way to document your computational experiments.\n",
    "\n",
    "A notebook is structured into different _cells_. Each cell contains either (executable) code, an output, or documention (text, images, links ...).\n",
    "\n",
    "The most important shortcut for you is: <kbd>Shift</kbd> + <kbd>Return</kbd> \n",
    "This will execute the code in a cell.\n",
    "\n",
    "There are many nice tutorials online. If you want to have a closer look at the notebook interface, we would recommend:\n",
    "\n",
    "- [Tips and Tricks](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)\n",
    "- [A longer tutorial](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook#gs.Qb=Px60)\n",
    "\n",
    "A quick test: just enter some simple calculations into the cell below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1 + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Overview\n",
    "\n",
    "||continuous|categorical|\n",
    "|---|---|---|\n",
    "|**supervised**|**regression**|classification|\n",
    "|**unsupervised**|dimension reduction|clustering|\n",
    "\n",
    "\n",
    "Regression is the task of predicting a continuous (e.g. real-valued) target y from a set of observations x. It is one of the protypical examples of supervised learning. Some simple cases of regression tasks:\n",
    "\n",
    "- predict temperature\n",
    "- predict prices\n",
    "- predict signal value at a single voxel in an MR scan.\n",
    "\n",
    "Linear regression models are a good starting point for regression tasks.\n",
    "Such models are popular because they can be fit very quickly, and are very interpretable.\n",
    "\n",
    "<img src=\"./img/linear_fit.png\" align=\"middle\"/>\n",
    "\n",
    "\n",
    "You are probably familiar with the simplest form of a linear regression model (i.e., fitting a straight line to data) but such models can be extended to model more complicated data behavior. The linear regression model can be used as long as long as we have a linear function of the _coefficients_, so we can actually fit more complex relationships between x and y. This is done using **basis functions**. These are functions $\\phi(x)$ that transform the input variable x and linear regression is done on the transformed $\\phi(x)$. Today we will look the simplest case: polynomial kernels, where $\\phi_n(x) = x^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more complex your model is, the greater is the risk of overfitting the data. As can be nicely illustrated by using polynomial models of increasing order. The actual data is shown as blue points, the actual function underlying the data is shown in blue and the estimated polynomial model in green.\n",
    "\n",
    "![](./img/polynomial-fit-complexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see from the plots above, there seems to be a sweet spot between too simple and too complex models. But how to find the right complexity? Well, here is one way:\n",
    "\n",
    "You can look at the error on the training and on the test data, to see at which point overfitting occurs. If you use simple models, your training and test error will typically be high. Both will decrease with increasing model compexity. But: at a certain model complexity, the test set error starts to increase again. This is where you would start to overfit your training data. The plots show the model complexity against mean squared error (mse). \"Good\" models have low training error and a similar test error. (The goals are: \"decrease the training error and decrease the gap between training and test error.\") In this particular example, polynomial models of orders 6, 7, or 8 would seem like a good choice...\n",
    "![](./img/polynomial-fit-overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, when you use more complex models, you should also use regularization to reduce overfitting. Regularizing typically forces your parameters to be small and your results to be smoother and less variable. We will explore this in a bit more detail later...\n",
    "![](./img/polynomial-fit-regularizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But you should always keep another option in mind: The more data you collect, the better your fit will be. Collecting more data also regularizes your model fit and reduces the risk of overfitting.\n",
    "![](./img/polynomial-fit-data-regularization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will start with a quick intuitive walk-through of the mathematics behind this well-known problem, before seeing how before moving on to see how linear models can be generalized to account for more complicated patterns in data.\n",
    "\n",
    "So, why are we learning linear regression?\n",
    "- widely used\n",
    "- runs fast\n",
    "- easy to use (not a lot of tuning required)\n",
    "- highly interpretable\n",
    "- basis for many other methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating (\"Learning\") Model Coefficients\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we are find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/08_estimating_coefficients.png\" style=\"width: 600px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What elements are present in the diagram?\n",
    "- The black dots are the <span style=\"color:black;\">**observed values**</span> of x and y. \n",
    "- The blue line is our <span style=\"color:blue;\">**least squares line**</span>.\n",
    "- The red lines are the <span style=\"color:red;\">**residuals**</span>, which are the distances between the observed values and the least squares line.\n",
    "\n",
    "How do the model coefficients relate to the least squares line?\n",
    "- $w_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "- $w_1$ is the **slope** (the change in $y$ divided by change in $x$)\n",
    "- with $y = w_1 x + w_0$\n",
    "\n",
    "Here is a graphical depiction of those calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/08_slope_intercept.png\" style=\"width: 400px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "We will start with the most familiar linear regression, a straight-line fit to data.\n",
    "A straight-line fit is a model of the form $y = w_1 x + w_0$\n",
    "\n",
    "where $w_1$ is commonly known as the *slope*, and $w_0$ is commonly known as the *intercept*.\n",
    "\n",
    "Consider the following data, which is scattered about a line with a slope of 2 and an intercept of -5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "t = 2 * x - 5 + rng.randn(50)\n",
    "plt.scatter(x, t);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Scikit-Learn's ``LinearRegression`` estimator to fit this data and construct the best-fit line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], t)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, t)\n",
    "plt.plot(xfit, yfit);\n",
    "plt.savefig('./img/linear_fit.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope and intercept of the data are contained in the model's fit parameters, which in Scikit-Learn are always marked by a trailing underscore.\n",
    "Here the relevant parameters are ``coef_`` and ``intercept_``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Model slope:    \", model.coef_[0])\n",
    "print(\"Model intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are very close to the inputs, as we might hope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``LinearRegression`` estimator is much more capable than this, however—in addition to simple straight-line fits, it can also handle multidimensional linear models of the form\n",
    "$$\n",
    "y = w_0 + w_1 x_1 + w_2 x_2 + \\cdots\n",
    "$$\n",
    "where there are multiple $x$ values.\n",
    "Geometrically, this is akin to fitting a plane to points in three dimensions, or fitting a hyper-plane to points in higher dimensions.\n",
    "\n",
    "The multidimensional nature of such regressions makes them more difficult to visualize, but we can see one of these fits in action by building some example data, using NumPy's matrix multiplication operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = 10 * rng.rand(100, 3)\n",
    "t = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "\n",
    "model.fit(X, t)\n",
    "print(\"Model intercept:\", model.intercept_)\n",
    "print(\"Model slopes:   \", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the $t$ (target) data is constructed from three random $x$ values, and the linear regression recovers the coefficients used to construct the data.\n",
    "\n",
    "In this way, we can use the single ``LinearRegression`` estimator to fit lines, planes, or hyperplanes to our data.\n",
    "It still appears that this approach would be limited to strictly linear relationships between variables, but it turns out we can relax this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis Function Regression\n",
    "\n",
    "One trick you can use to adapt linear regression to nonlinear relationships between variables is to transform the data according to *basis functions*.\n",
    "We will use the ``PolynomialRegression`` pipeline..\n",
    "The idea is to take our multidimensional linear model:\n",
    "$$\n",
    "y = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3 + \\cdots\n",
    "$$\n",
    "and build the $x_1, x_2, x_3,$ and so on, from our single-dimensional input $x$.\n",
    "That is, we let $x_n = f_n(x)$, where $f_n()$ is some function that transforms our data.\n",
    "\n",
    "For example, if $f_n(x) = x^n$, our model becomes a polynomial regression:\n",
    "$$\n",
    "y = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\cdots\n",
    "$$\n",
    "Notice that this is *still a linear model*—the linearity refers to the fact that the coefficients $w_n$ never multiply or divide each other.\n",
    "What we have effectively done is taken our one-dimensional $x$ values and projected them into a higher dimension, so that a linear fit can fit more complicated relationships between $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial basis functions\n",
    "\n",
    "This polynomial projection is useful enough that it is built into Scikit-Learn, using the ``PolynomialFeatures`` transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "x = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(x[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the transformer has converted our one-dimensional array into a three-dimensional array by taking the exponent of each value.\n",
    "This new, higher-dimensional data representation can then be plugged into a linear regression.\n",
    "\n",
    "The cleanest way to accomplish this is to use a pipeline.\n",
    "Let's make a 7th-degree polynomial model in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "poly_model = make_pipeline(PolynomialFeatures(7),\n",
    "                           LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this transform in place, we can use the linear model to fit much more complicated relationships between $x$ and $y$. \n",
    "For example, here is a sine wave with noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "t = np.sin(x) + 0.1 * rng.randn(50)\n",
    "poly_model.fit(x[:, np.newaxis], t)\n",
    "\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, t)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our linear model, through the use of 7th-order polynomial basis functions, can provide an excellent fit to this non-linear data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "The introduction of basis functions into our linear regression makes the model much more flexible, but it also can very quickly lead to over-fitting (refer back to [Hyperparameters and Model Validation](05.03-Hyperparameters-and-Model-Validation.ipynb) for a discussion of this).\n",
    "For example, if we choose too many Gaussian basis functions, we end up with results that don't look so good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(PolynomialFeatures(16),\n",
    "                      LinearRegression())\n",
    "model.fit(x[:, np.newaxis], t)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, t)\n",
    "plt.plot(xfit, yfit)\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.5, 1.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data projected to the 16-dimensional basis, the model has far too much flexibility and goes to extreme values between locations where it is constrained by data.\n",
    "We can see the reason for this if we plot the coefficients of the polynomial bases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basis_plot(model, title=None):\n",
    "    fig, ax = plt.subplots(1,2, sharex=False, figsize=(20,5))\n",
    "    model.fit(x[:, np.newaxis], t)\n",
    "    ax[0].scatter(x, t)\n",
    "    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "    ax[0].set(xlabel='x', ylabel='y', ylim=(-1.5, 1.5))\n",
    "    \n",
    "    if title:\n",
    "        ax[0].set_title(title)\n",
    "\n",
    "    ax[1].plot(model.steps[0][1].powers_,\n",
    "               model.steps[1][1].coef_)\n",
    "    ax[1].set(xlabel='basis power',\n",
    "              ylabel='coefficient')\n",
    "    \n",
    "model = make_pipeline(PolynomialFeatures(16), LinearRegression())\n",
    "basis_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left panel of this figure shows the amplitude of the basis function for each power of x.\n",
    "This is typical over-fitting behavior: the coefficients of higher order basis functions oscillate and cancel each other out.\n",
    "We know that such behavior is problematic, and it would be nice if we could limit such spikes expliticly in the model by penalizing large values of the model parameters.\n",
    "Such a penalty is known as *regularization*, and comes in several forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Ridge regression ($L_2$ Regularization)\n",
    "\n",
    "Perhaps the most common form of regularization is known as *ridge regression* or $L_2$ *regularization*, sometimes also called *Tikhonov regularization*.\n",
    "This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be \n",
    "$$\n",
    "P = \\alpha\\sum_{n=1}^N w_n^2\n",
    "$$\n",
    "where $\\alpha$ is a free parameter that controls the strength of the penalty.\n",
    "This type of penalized model is built into Scikit-Learn with the ``Ridge`` estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "model = make_pipeline(PolynomialFeatures(16), Ridge(alpha=0.08))\n",
    "basis_plot(model, title='Ridge Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\alpha$ parameter is essentially a knob controlling the complexity of the resulting model.\n",
    "In the limit $\\alpha \\to 0$, we recover the standard linear regression result; in the limit $\\alpha \\to \\infty$, all model responses will be suppressed. Just play with different values of $\\alpha$ and see how the resulting fit changes, and what happens to the weights of the coefficients in the lower panel.\n",
    "\n",
    "One advantage of ridge regression in particular is that it can be computed very efficiently—at hardly more computational cost than the original linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression ($L_1$ regularization)\n",
    "\n",
    "Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:\n",
    "$$\n",
    "P = \\alpha\\sum_{n=1}^N |w_n|\n",
    "$$\n",
    "Though this is conceptually very similar to ridge regression, the results can differ surprisingly: for example, due to geometric reasons lasso regression tends to favor *sparse models* where possible: that is, it preferentially sets model coefficients to exactly zero.\n",
    "\n",
    "We can see this behavior in duplicating the ridge regression figure, but using L1-normalized coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model = make_pipeline(PolynomialFeatures(16), Lasso(alpha=0.001, max_iter=100000))\n",
    "basis_plot(model, title='Lasso Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the lasso regression penalty, the majority of the coefficients are exactly zero, with the functional behavior being modeled by a small subset of the available basis functions.\n",
    "As with ridge regularization, the $\\alpha$ parameter tunes the strength of the penalty, and should be determined via, for example, cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance and Bias\n",
    "\n",
    "Let's have a look again at the problem of bias and variance of a Machine Learning model. This is related to the problem of underfitting and overfitting. The more complex your model is, the more variation in the data you can fit. Unfortunately, this also includes noise in the training data, i.e. variations that are not coming from the underlying signal you are interested in. On the other hand, if your model cannot represent the signal, it will fail to fit the general shape of your data (think a linear model fitted to a nonlinear function such as the sine function).\n",
    "\n",
    "a reminder from Wikipedia: In statistics and machine learning, the bias–variance tradeoff (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:\n",
    "- The bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n",
    "- The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By playing with the number of polynomial basis functions (e.g. $x, x^2, x^3, ...$), also called the degree of the polynomial, you can produce fits of varying complexity to the data. The degree can be changed via the **PolynomialFeatures(n)** option. You can play with this option below and observe the under- or over-fitting behaviour of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_model = make_pipeline(PolynomialFeatures(16),\n",
    "                           LinearRegression())\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "t = np.sin(x) + 0.1 * rng.randn(50)\n",
    "\n",
    "poly_model.fit(x[:, np.newaxis], t)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, t)\n",
    "plt.plot(xfit, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the variance and the bias of the estimate, we can sample a number of datasets and compute how well the model fits and how much the fitted coefficients vary:\n",
    "- To analyze the bias of the model, we compute the mean squared error between prediction and the target value y.\n",
    "- To visualize the variance we can look at the variation of the fitted models. In order to compare models of different polynomial degrees, we will only look at the first coefficient for x (i.e. $w_1$) which should exist in all models.\n",
    "\n",
    "You can experiment with the code below by changing:\n",
    "- The complexity of the model, i.e. the number of polynomial kernels: ```n_polynomial```\n",
    "    - ```n_polynomial = 1``` gives us just the linear model\n",
    "    - ```n_polynomial = 9``` uses coefficients $x, x^2, x^3, ..., x^9$ etc..\n",
    "- With ```n_samples``` you can change the number of data points used in each experiment.\n",
    "- With ```n_experiments``` you can set the number of experiments, i.e. how often you want to draw a random dataset and fit the model.\n",
    "\n",
    "The code below will output two histograms: the left plot shows the variation of $w_1$ across the experiments; the right plot shows how the mean-squared-error is distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a linear model (a polynomial of degree 1). What would you expect the bias and variance to look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_polynomial = 1\n",
    "n_samples = 50\n",
    "n_experiments = 200\n",
    "\n",
    "poly_model = make_pipeline(PolynomialFeatures(n_polynomial),\n",
    "                           LinearRegression())\n",
    "\n",
    "fitted_coeffs = np.zeros((n_experiments,n_polynomial+1))\n",
    "mse = np.zeros(n_experiments)\n",
    "\n",
    "for i in range(n_experiments):\n",
    "    rng = np.random.RandomState()\n",
    "    x = 10 * rng.rand(n_samples)\n",
    "    t = np.sin(x) + 0.1 * rng.randn(n_samples)\n",
    "\n",
    "    poly_model.fit(x[:, np.newaxis], t)\n",
    "    fitted_coeffs[i] = poly_model.steps[1][1].coef_\n",
    "    yfit=poly_model.predict(x[:,np.newaxis])\n",
    "    mse[i]=np.sum((yfit-t)**2)\n",
    "\n",
    "plt.figure(1)\n",
    "ax1=plt.subplot(121)\n",
    "ax1.set_title(\"coefficient w1\")\n",
    "plt.hist(fitted_coeffs[:,1],30,range=[-2, 2])\n",
    "ax2=plt.subplot(122)\n",
    "ax2.set_title(\"mean squared error of prediction\")\n",
    "plt.hist(mse, 30, range=[0,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use a more complex model (a polynomial of degree 9). Will it have less or more bias (i.e. would it fit each sampled dataset better or worse)? What about the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_polynomial = 9\n",
    "n_samples = 50\n",
    "n_experiments = 200\n",
    "\n",
    "poly_model = make_pipeline(PolynomialFeatures(n_polynomial),\n",
    "                           LinearRegression())\n",
    "\n",
    "fitted_coeffs = np.zeros((n_experiments,n_polynomial+1))\n",
    "mse = np.zeros(n_experiments)\n",
    "\n",
    "for i in range(n_experiments):\n",
    "    rng = np.random.RandomState()\n",
    "    x = 10 * rng.rand(n_samples)\n",
    "    t = np.sin(x) + 0.1 * rng.randn(n_samples)\n",
    "\n",
    "    poly_model.fit(x[:, np.newaxis], t)\n",
    "    fitted_coeffs[i] = poly_model.steps[1][1].coef_\n",
    "    yfit=poly_model.predict(x[:,np.newaxis])\n",
    "    mse[i]=np.sum((yfit-t)**2)\n",
    "\n",
    "plt.figure(1)\n",
    "ax1=plt.subplot(121)\n",
    "ax1.set_title(\"coefficient w1\")\n",
    "plt.hist(fitted_coeffs[:,1],30,range=[-5, 5])\n",
    "ax2=plt.subplot(122)\n",
    "ax2.set_title(\"mean squared error of prediction\")\n",
    "plt.hist(mse, 30, range=[0,50])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the simple linear model doesn't fit the data well: it has a high bias. But the coefficients remain stable when you sample new data and fit again (low variance). The complex model accurately fits each sampled dataset (too good actually): it has a low bias. But the estimated coefficients vary a lot between each dataset you sample, so this indicates overfitting: If you have to change your parameters to each new dataset, you probably didn't capture the underlying structure of the data. Those overfitted models with very well approximate your training data but they won't predict new (unseen) data very well. The best way to estimate the expected behavior of models on new data is by using a separate training set (for fitting the model) and a test set (to validate its generalization performance). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### things to play with...\n",
    "\n",
    "- Test models of varying complexity and compare the results.\n",
    "- Increase number of samples used for fitting.\n",
    "    - How does it influence overfitting for polynomials of higher order?\n",
    "- Create a separate training and test set.\n",
    "    - Use the training set to fit the model and the test set to validate the performance.\n",
    "    - Test polyomial regression models of increasing order. How do the different models generalize (i.e. how well do they perform on the test set)?\n",
    "    - How does regularization influence the generalization of your models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Predicting Bicycle Traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As an example, let's take a look at whether we can predict the number of bicycle trips across Seattle's Fremont Bridge based on weather, season, and other factors.\n",
    "\n",
    "We will join the bike data with another dataset, and try to determine the extent to which weather and seasonal factors—temperature, precipitation, and daylight hours—affect the volume of bicycle traffic through this corridor.\n",
    "Fortunately, the NOAA makes available their daily [weather station data](http://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND) (I used station ID USW00024233) and we can easily use Pandas to join the two data sources.\n",
    "We will perform a simple linear regression to relate weather and other information to bicycle counts, in order to estimate how a change in any one of these parameters affects the number of riders on a given day.\n",
    "\n",
    "In particular, this is an example of how the tools of Scikit-Learn can be used in a statistical modeling framework, in which the parameters of the model are assumed to have interpretable meaning.\n",
    "As discussed previously, this is not a standard approach within machine learning, but such interpretation is possible for some models.\n",
    "\n",
    "Let's start by loading the two datasets, indexing by date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !curl -o FremontBridge.csv https://data.seattle.gov/api/views/65db-xm6k/rows.csv?accessType=DOWNLOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/fremont-bridge.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "daily = pd.read_csv('data/daily_counts_weather.csv', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in place, we can choose the columns to use, and fit a linear regression model to our data.\n",
    "We will set ``fit_intercept = False``, because the daily flags essentially operate as their own day-specific intercepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop any rows with null values\n",
    "daily.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "#put in the columns you want to use as predictor variables, for example column_names = ['daylight_hrs','holiday','PRCP'] or column_names=list(daily.columns) for all entries\n",
    "\n",
    "column_names = ['daylight_hrs']\n",
    "\n",
    "\n",
    "X = daily[column_names]\n",
    "y = daily['Total']\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model.fit(X, y)\n",
    "daily['predicted'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compare the total and predicted bicycle traffic visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily[['Total', 'predicted']].plot(alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple regression on the hours of daylight at least partially explains the general trend. Now just go back and add more predictor variables and see how the fit changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that we have missed some key features, especially during the summer time.\n",
    "Either our features are not complete (i.e., people decide whether to ride to work based on more than just these) or there are some nonlinear relationships that we have failed to take into account (e.g., perhaps people ride less at both high and low temperatures).\n",
    "Nevertheless, our rough approximation is enough to give us some insights, and we can take a look at the coefficients of the linear model to estimate how much each feature contributes to the daily bicycle count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = pd.Series(model.coef_, index=X.columns)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers are difficult to interpret without some measure of their uncertainty.\n",
    "We can compute these uncertainties quickly using bootstrap resamplings of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "np.random.seed(1)\n",
    "err = np.std([model.fit(*resample(X, y)).coef_\n",
    "              for i in range(1000)], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these errors estimated, let's again look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pd.DataFrame({'effect': params.round(0),\n",
    "                    'error': err.round(0)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect column will show you for each variable how the total number of people taking the bike changes if you change the variable by 1 unit. E.g. for each additional hour of daylight there are 223 more people (on their bikes) crossing the bridge.  \n",
    "\n",
    "Our model is almost certainly missing some relevant information. For example, nonlinear effects (such as effects of precipitation *and* cold temperature) and nonlinear trends within each variable (such as disinclination to ride at very cold and very hot temperatures) cannot be accounted for in this model.\n",
    "Additionally, we have thrown away some of the finer-grained information (such as the difference between a rainy morning and a rainy afternoon), and we have ignored correlations between days (such as the possible effect of a rainy Tuesday on Wednesday's numbers, or the effect of an unexpected sunny day after a streak of rainy days).\n",
    "These are all potentially interesting effects, and you now have the tools to begin exploring them if you wish!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional topics (for those who are interested)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 - Gaussian basis functions\n",
    "\n",
    "Of course, other basis functions are possible.\n",
    "For example, one useful pattern is to fit a model that is not a sum of polynomial bases, but a sum of Gaussian bases.\n",
    "The result might look something like the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/05.06-gaussian-basis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shaded regions in the plot are the scaled basis functions, and when added together they reproduce the smooth curve through the data.\n",
    "These Gaussian basis functions are not built into Scikit-Learn, but we can write a custom transformer that will create them, as shown here and illustrated in the following figure (Scikit-Learn transformers are implemented as Python classes; reading Scikit-Learn's source is a good way to see how they can be created):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GaussianFeatures(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Uniformly spaced Gaussian features for one-dimensional input\"\"\"\n",
    "    \n",
    "    def __init__(self, N, width_factor=2.0):\n",
    "        self.N = N\n",
    "        self.width_factor = width_factor\n",
    "    \n",
    "    @staticmethod\n",
    "    def _gauss_basis(x, y, width, axis=None):\n",
    "        arg = (x - y) / width\n",
    "        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # create N centers spread along the data range\n",
    "        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n",
    "        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return self._gauss_basis(X[:, :, np.newaxis], self.centers_,\n",
    "                                 self.width_, axis=1)\n",
    "    \n",
    "gauss_model = make_pipeline(GaussianFeatures(20),\n",
    "                            LinearRegression())\n",
    "gauss_model.fit(x[:, np.newaxis], y)\n",
    "yfit = gauss_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)\n",
    "plt.xlim(0, 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put this example here just to make clear that there is nothing magic about polynomial basis functions: if you have some sort of intuition into the generating process of your data that makes you think one basis or another might be appropriate, you can use them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = make_pipeline(GaussianFeatures(30),\n",
    "                      LinearRegression())\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylim(-1.5, 1.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basis_plot_gauss(model, title=None):\n",
    "    fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "    model.fit(x[:, np.newaxis], y)\n",
    "    ax[0].scatter(x, y)\n",
    "    ax[0].plot(xfit, model.predict(xfit[:, np.newaxis]))\n",
    "    ax[0].set(xlabel='x', ylabel='y')\n",
    "    \n",
    "    if title:\n",
    "        ax[0].set_title(title)\n",
    "\n",
    "    ax[1].plot(model.steps[0][1].centers_,\n",
    "               model.steps[1][1].coef_)\n",
    "    ax[1].set(xlabel='basis location',\n",
    "              ylabel='coefficient')\n",
    "    \n",
    "model = make_pipeline(GaussianFeatures(30), LinearRegression())\n",
    "basis_plot_gauss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### things to test\n",
    "\n",
    "- Use regularization as above in the polynomial examples.\n",
    "- What happens to the coefficients when you use regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 - Create data frames with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "counts = pd.read_csv('data/FremontBridge.csv', index_col='Date', parse_dates=True)\n",
    "weather = pd.read_csv('data/BicycleWeather.csv', index_col='DATE', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a quick overview of the data. Each pandas dataframe has the function .head() that shows the first couple of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will compute the total daily bicycle traffic, and put this in its own dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily = counts.resample('d').sum()\n",
    "daily['Total'] = daily.sum(axis=1)\n",
    "daily = daily[['Total']] # remove other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing the counts for each day can be done in pandas using _resample_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily=counts.resample('d').sum()\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily['Total'] = daily.sum(axis=1) #sum counts for both columns (i.e. both sides of the bridge)\n",
    "daily = daily[['Total']] # remove other columns\n",
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The patterns of use generally vary from day to day; let's account for this in our data by adding binary columns that indicate the day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "for i in range(7):\n",
    "    daily[days[i]] = (daily.index.dayofweek == i).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we might expect riders to behave differently on holidays; let's add an indicator of this as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "holidays = cal.holidays('2012', '2016')\n",
    "daily = daily.join(pd.Series(1, index=holidays, name='holiday'))\n",
    "daily['holiday'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also might suspect that the hours of daylight would affect how many people ride; let's use the standard astronomical calculation to add this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hours_of_daylight(date, axis=23.44, latitude=47.61):\n",
    "    \"\"\"Compute the hours of daylight for the given date\"\"\"\n",
    "    days = (date - pd.datetime(2000, 12, 21)).days\n",
    "    m = (1. - np.tan(np.radians(latitude))\n",
    "         * np.tan(np.radians(axis) * np.cos(days * 2 * np.pi / 365.25)))\n",
    "    return 24. * np.degrees(np.arccos(1 - np.clip(m, 0, 2))) / 180.\n",
    "\n",
    "daily['daylight_hrs'] = list(map(hours_of_daylight, daily.index))\n",
    "daily[['daylight_hrs']].plot()\n",
    "plt.ylim(8, 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add the average temperature and total precipitation to the data.\n",
    "In addition to the inches of precipitation, let's add a flag that indicates whether a day is dry (has zero precipitation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temperatures are in 1/10 deg C; convert to C\n",
    "weather['TMIN'] /= 10\n",
    "weather['TMAX'] /= 10\n",
    "\n",
    "weather['Temp (C)'] = 0.5 * (weather['TMIN'] + weather['TMAX'])\n",
    "\n",
    "# precip is in 1/10 mm; convert to inches\n",
    "weather['PRCP'] /= 254\n",
    "weather['dry day'] = (weather['PRCP'] == 0).astype(int)\n",
    "\n",
    "daily = daily.join(weather[['PRCP', 'Temp (C)', 'dry day']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's add a counter that increases from day 1, and measures how many years have passed.\n",
    "This will let us measure any observed annual increase or decrease in daily crossings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily['annual'] = (daily.index - daily.index[0]).days / 365."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is in order, and we can take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daily.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_model = make_pipeline(PolynomialFeatures(0),\n",
    "                           LinearRegression())\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "x_train = np.linspace(0,10,10)\n",
    "y_train = np.sin(x_train) + 0.1 * rng.randn(10)\n",
    "\n",
    "rng = np.random.RandomState(2)\n",
    "x_test = np.linspace(1,8,10)\n",
    "y_test = np.sin(x_test) + 0.1 * rng.randn(10)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "\n",
    "poly_model.fit(x_train[:, np.newaxis], y_train)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(xfit, np.sin(xfit))\n",
    "plt.plot(xfit, yfit)\n",
    "plt.savefig('./img/polynomial-fit-sine-order-0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_model = make_pipeline(PolynomialFeatures(1),\n",
    "                           LinearRegression())\n",
    "\n",
    "poly_model.fit(x_train[:, np.newaxis], y_train)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(xfit, np.sin(xfit))\n",
    "plt.plot(xfit, yfit)\n",
    "plt.savefig('./img/polynomial-fit-sine-order-1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_model = make_pipeline(PolynomialFeatures(4),\n",
    "                           LinearRegression())\n",
    "\n",
    "poly_model.fit(x_train[:, np.newaxis], y_train)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(xfit, np.sin(xfit))\n",
    "plt.plot(xfit, yfit)\n",
    "plt.savefig('./img/polynomial-fit-sine-order-4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_model = make_pipeline(PolynomialFeatures(9),\n",
    "                           LinearRegression())\n",
    "\n",
    "poly_model.fit(x_train[:, np.newaxis], y_train)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(xfit, np.sin(xfit))\n",
    "plt.plot(xfit, yfit)\n",
    "plt.savefig('./img/polynomial-fit-sine-order-9.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_model(degree):\n",
    "    poly_model = make_pipeline(PolynomialFeatures(degree),\n",
    "                           LinearRegression())\n",
    "\n",
    "    poly_model.fit(x_train[:, np.newaxis], y_train)\n",
    "    y_fit = poly_model.predict(x_train[:, np.newaxis])\n",
    "    y_fit_test = poly_model.predict(x_test[:,np.newaxis])\n",
    "    return np.asarray([np.sum((y_fit-y_train)**2), np.sum((y_fit_test-y_test)**2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res=np.asarray([validate_model(i) for i in range(0,13)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res=np.transpose(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.plot(res[0,:])\n",
    "plt.plot(res[1,:])\n",
    "plt.legend(('training error','test error'))\n",
    "plt.savefig('img/polynomial-fit-training-vs-test.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly_model = make_pipeline(PolynomialFeatures(9),\n",
    "                           LinearRegression())\n",
    "n_samples=100\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "x_train = np.linspace(0,10,n_samples)\n",
    "y_train = np.sin(x_train) + 0.1 * rng.randn(n_samples)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "\n",
    "poly_model.fit(x_train[:, np.newaxis], y_train)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(xfit, np.sin(xfit))\n",
    "plt.plot(xfit, yfit)\n",
    "plt.savefig('./img/polynomial-fit-sine-order-9-'+str(n_samples) +'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "alpha = 0.1\n",
    "\n",
    "poly_model = make_pipeline(PolynomialFeatures(9),\n",
    "                          Ridge(alpha=alpha))\n",
    "\n",
    "n_samples=10\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "\n",
    "x_train = np.linspace(0,10,n_samples)\n",
    "y_train = np.sin(x_train) + 0.1 * rng.randn(n_samples)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "\n",
    "poly_model.fit(x_train[:, np.newaxis], y_train)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(xfit, np.sin(xfit))\n",
    "plt.plot(xfit, yfit)\n",
    "plt.savefig('./img/polynomial-fit-sine-order-9-'+str(n_samples) +'-regularized-'+str(alpha) +'.png')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nteract": {
   "version": "0.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
