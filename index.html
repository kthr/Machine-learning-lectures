<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>An introduction to Machine Learning...</title>
        <link rel="stylesheet" href="./css/reveal.css">
        <link rel="stylesheet" href="./css/theme/black.css" id="theme">
        <link rel="stylesheet" href="./css/highlight/zenburn.css">
        <link rel="stylesheet" href="./css/print/paper.css" type="text/css" media="print">

    </head>
    <body>

        <div class="reveal">
            <div class="slides"><section  data-markdown><script type="text/template">

# What is Machine Learning?
</script></section><section ><section data-markdown><script type="text/template">
<!-- .slide: data-background="https://cdn.redshift.autodesk.com/2016/05/Machine-Learning-hero.jpg" -->  

<aside class="notes"><p>An integral part of Artificial Intelligence</p>
</aside></script></section><section data-markdown><script type="text/template">
## AI: a long history
  
<img src="https://upload.wikimedia.org/wikipedia/commons/0/0b/Medeia_and_Talus.png" height=500>
</script></section><section data-markdown><script type="text/template">
## AI: a long history

> The Analytical Engine has no pretensions whatever to originate anything.	It can do whatever we know how to order it to perform... --- Ada Lovelace 1843

- The Imitation Game - Alan Turing "Computing Machinery and Intelligence", 1950
</script></section><section data-markdown><script type="text/template">

AI solved problems that are intellectually difficult for humans

<img src="https://images.chesscomfiles.com/uploads/v1/blog/299228.1a04250c.5000x5000o.9607e919a884.jpeg" height = 500 >
</script></section><section data-markdown><script type="text/template">
The true challenge: tasks that are easy for people to do but hard to describe formally 
<img src="https://www.peonderey.com/wp-content/uploads/2016/05/13242063_10154042837576675_1639904989_o.jpg" height = 500 >
</script></section><section data-markdown><script type="text/template">
## Machine Learning and AI

- evolved in the 50s and 60s from AI, pattern recognition and computational learning theory	
- learn from data (and predict)


<aside class="notes"><ul>
<li>grew out of quest for AI<ul>
<li>however, AI was dominated by logical, knowledge-based approaches </li>
<li>flourished in the 1990s<ul>
<li>from AI to tackle practical problems using data </li>
</ul>
</li>
<li>used where explicit solutions are hard to find:<ul>
<li>OCR, SPAM, Network intrusion, Computer Vision</li>
</ul>
</li>
</ul>
</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">
- 1763 | Bayes 
- 1913 | Markov chains
- 1950 | Turing “Computing Machinery and Intelligence.” Mind.
- 1957 | Rosenblatt - Perceptron 
- 1980 | Fukushima - Neocognitron 
- 1982 | Hopfield - Recurrent Networks 
- 1986 | Rumelhart, Hinton, Williams: Backpropagation 
- 1995 | Ho - Random Forest 
- 1995 | Cortes and Vapnik - Support Vector Machines
- 1997 | Hochreiter and Schmidhuber - LSTM networks
</script></section><section data-markdown><script type="text/template">
## Knowledge from data

<img src="https://upload.wikimedia.org/wikipedia/commons/d/da/Brahe_notebook.jpg" height=500 >

<aside class="notes"><p>Tycho Brahe&#39;s careful observations were used by Kepler to derive his three laws of planetary motion</p>
</aside></script></section><section data-markdown><script type="text/template">
## Knowledge from data 

 
> We are drowning in information and starving for knowledge. — John Naisbitt.

<aside class="notes"><p>Today we need computerised methods for most problems, <!-- .slide: data-background="https://upload.wikimedia.org/wikipedia/commons/6/69/NASA-HS201427a-HubbleUltraDeepField2014-20140603.jpg"--></p>
</aside></script></section><section data-markdown><script type="text/template">
## Machine Learning:

> ...a set of methods that can automatically detect patterns in data, and then *use the uncovered patterns to predict future data*, or to perform other kinds of decision making under uncertainty. -- Murphy 2012, Machine Learning
</script></section><section data-markdown><script type="text/template">
- Rules, Data  -> [Programming] -> Answers
- Data, Answers -> [ML] -> Rules <!-- .element: class="fragment" -->
</script></section><section data-markdown><script type="text/template">
## Prerequisites:

- There is a pattern in the data
- We cannot write down a mathematical model of it
- There is (enough) data
</script></section><section data-markdown><script type="text/template">
## But then you can do a lot...
</script></section><section data-markdown><script type="text/template">
### colorization

<img src=https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/07/Colorization-of-Black-and-White-Photographs.png height = 500>
</script></section><section data-markdown><script type="text/template">
### detection 

<img src=http://cs.stanford.edu/people/karpathy/deepimagesent/dogball.png height = 500>
</script></section><section data-markdown><script type="text/template">
### image captioning

<img src=https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/07/Automatic-Image-Caption-Generation.png height = 500>
</script></section><section data-markdown><script type="text/template">
### image translation

<img src=https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/07/Instant-Visual-Translation.png>
</script></section><section data-markdown><script type="text/template">
### face generation 

<img src=https://camo.githubusercontent.com/d371bad7ae1f9f5bf9e0f1906e46726e503ab99d/687474703a2f2f692e696d6775722e636f6d2f45334d677a6e422e6a7067>
</script></section><section data-markdown><script type="text/template">
### automated stylised drawing

<img src=https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/07/Automatically-Create-Styled-Image-From-Sketch.png height>
</script></section></section><section  data-markdown><script type="text/template">
## Machine Learning: A Taxonomy

> A computer program is said to *learn* from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. -- from Mitchell, T. (1997). Machine Learning. McGraw Hill. 
</script></section><section ><section data-markdown><script type="text/template">
## The Task, T
> A computer program is said to *learn* from experience E with respect to some class of **tasks T** and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. -- from Mitchell, T. (1997). Machine Learning. McGraw Hill. 
</script></section><section data-markdown><script type="text/template">
## The Task, T

- ML is interesting for tasks that are too difficult to solve with fixed programs 
</script></section><section data-markdown><script type="text/template">
## Classification

- learn mapping from $\vec{x}$ to $y \in \\{ 1, ..., C \\} $
- or probability distribution over classes
- possibly with missing inputs (e.g. medical records)

<aside class="notes"><ul>
<li>response variable y categorical<pre><code>          - C = 2 binary
          - C &gt; 2 multiclass
</code></pre></li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">
## Classification examples

- email spam filtering
- object recognition / image classification
	- handwriting recognition
	- face detection 
</script></section><section data-markdown><script type="text/template">
The drosophila of Machine Learning:
<img src="https://raw.github.com/nscherf/01-ML-introduction/gh-pages/img/mnist.png" height=500 >

 
</script></section><section data-markdown><script type="text/template">
## Regression

- predict numerical value given input
- approximate $ y = f(\vec{x}) $ by $ \hat{y} = \hat{f}(\vec{x}) $

<aside class="notes"><ul>
<li>response variable real-valued</li>
</ul>
</aside></script></section><section data-markdown><script type="text/template">
## Regression examples

- price of item (e.g. used car) based on covariates
- predict stock market tomorrow
- level of biomarker given clinical measurements
</script></section><section data-markdown><script type="text/template">
## Regression examples

<img src="https://upload.wikimedia.org/wikipedia/commons/7/7b/Gaussian_Kernel_Regression.png" height = 300 >
</script></section><section data-markdown><script type="text/template">
## Other tasks

- Structured output (a vector with relationships between elements) 
	- transcription (e.g. image to text)
	- translation (e.g. Klingon to French)
- Anomaly detection (e.g. credit card fraud)
</script></section><section data-markdown><script type="text/template">
## Other tasks 

- Synthesis and sampling (e.g. textures in video games)
- Imputaton of missing values: $ p(x\_i | x\_{-i})$
- Denoising: clean $x$ from corrupted $ \tilde{x} $: $ p(x| \widetilde{x}) $
- Density estimation: structure $p(x)$ from data p(x)
</script></section></section><section ><section data-markdown><script type="text/template">
## The Performance Measure, P

> A computer program is said to *learn* from experience E with respect to some class of tasks T and **performance measure P** if its performance at tasks in T, as measured by P, improves with experience E. -- from Mitchell, T. (1997). Machine Learning. McGraw Hill. 
</script></section><section data-markdown><script type="text/template">
## The Performance Measure, P

- classification: accuracy, error rate (expected 0-1 loss)
- density estimation: average (log) probability of examples
- depends on application
	- what are problematic mistakes?
	- e.g. regression: regular medium mistakes or rare large mistakes

</script></section></section><section ><section data-markdown><script type="text/template">
## The Experience, E

> A computer program is said to *learn* from **experience E** with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. -- from Mitchell, T. (1997). Machine Learning. McGraw Hill. 
</script></section><section data-markdown><script type="text/template">
## The Experience, E

- *supervised* vs *unsupervised* methods
</script></section><section data-markdown><script type="text/template">
## Supervised Learning (predictive learning)

- learn from data containing features and associated label(s) (given by an "instructor")
- learn mapping from features to response variable
- conditional density estimation: learn $p(\vec{y}| \vec{x})$ given $\vec{x}$ and $\vec{y}$
</script></section><section data-markdown><script type="text/template">
## Unsupervised Learning (descriptive learning) 

- dataset containing many features
	- learn useful properties of structure 
	- find interesting patterns in the data
- knowledge discovery, density estimation 
- unconditional density estimation: learn $p(\vec{x})$ given $\vec{x}$
- requires no training labels

<aside class="notes"><p>Unsupervised learning might be much closer to how animals learn: &quot;When we’re learning to see, nobody’s telling us what the right answers are — we just look. Every so often, your mother says “that’s a dog”, but that’s very little information. You’d be lucky if you got a few bits of information — even one bit per second — that way. The brain’s visual system has 10^14 neural connections. And you only live for 10^9 seconds. So it’s no use learning one bit per second. You need more like 10^5 bits per second. And there’s only one place you can get that much information: from the input itself.&quot; — Geoffrey Hinton, 1996 (quoted in (Gorder 2006)).</p>
</aside></script></section><section data-markdown><script type="text/template">
## Supervised vs Unsupervised

- Not a formal distinction 
- unsupervised problem could be split into a sequence of supervised problems: $ p(\vec{x}) = p(x_1) p(x_2 | x_1) p(x_3 | x_1, x_2) ... $
- also works the other way around $ p(y|x) = \frac{p(x,y)}{(\sum_{y'} p(x,y'))}$
- i.e. most methods could do both supervised and unsupervised 
</script></section><section data-markdown><script type="text/template">
## Typical examples supervised learning

- regression
- classification,
- structured output,
- denoising
</script></section><section data-markdown><script type="text/template">
## Typical examples unsupervised Learning

- density estimation 
- clustering
- imputation 
- latent factor discovery / dimensionality reduction
- (graph) structure (i.e. influence/correlations between variables)
</script></section><section data-markdown><script type="text/template">
Clustering

<img src="https://upload.wikimedia.org/wikipedia/commons/7/76/Sidney_Hall_-_Urania%27s_Mirror_-_Lacerta%2C_Cygnus%2C_Lyra%2C_Vulpecula_and_Anser.jpg" height = 500>
</script></section><section data-markdown><script type="text/template">
Unsupervised Learning: Density Estimation
  
<img src="https://c2.staticflickr.com/4/3258/5851679238_23b1b2bafe_b.jpg" height = 500>
</script></section><section data-markdown><script type="text/template">
Dimensionality Reduction

<img src="https://blog.sourced.tech/post/lapjv/mnist_after.png" height = "500">

</script></section><section data-markdown><script type="text/template">
Dimensionality Reduction

<img src="https://c2.staticflickr.com/2/1501/26194566691_ef39f2c77b_b.jpg" height = "500">
</script></section><section data-markdown><script type="text/template">
## Beyond (un)supervised 

- semi-supervised learning
	- sparse labelling of dataset 
- multi-instance learning
	- only known if a collection contains example of class or not
	- individual members not labeled 
</script></section></section><section ><section data-markdown><script type="text/template">
## Reinforcement Learning

- learning successful behavioural strategies from occasional rewards
</script></section><section data-markdown><script type="text/template">
<iframe width="840" height="472" src="https://www.youtube.com/embed/L4KBBAwF_bE" frameborder="0" allowfullscreen></iframe>
</script></section></section><section ><section data-markdown><script type="text/template">
## Machine Learning: The hype

<img src="https://upload.wikimedia.org/wikipedia/commons/9/94/Gartner_Hype_Cycle.svg" height = "400" style="background-color:white;">
</script></section><section data-markdown><script type="text/template">
## Machine Learning vs. Optimisation

Fitting data (optimisation) is different from finding patterns that generalise to unseen examples (machine learning)
</script></section><section data-markdown><script type="text/template">

Machine Learning | Statistics
---|---
Networks, graphs | model
Weights | parameters
Learning | fitting
Supervised learning | regression/classification
Unsupervised learning | density estimation, clustering
Large grant = $1,000,000 | large grant = $50,000

<small> *from http://statweb.stanford.edu/~tibs/stat315a/glossary.pdf* </small>
</script></section><section data-markdown><script type="text/template">
## ML vs statistics 

- Breiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science: A Review Journal of the Institute of Mathematical Statistics 16 (3) 199–231.
</script></section></section><section ><section data-markdown><script type="text/template">
## Some basic concepts
</script></section><section data-markdown><script type="text/template">
<img src="https://raw.github.com/nscherf/01-ML-introduction/gh-pages/img/learning-algorithm-scheme.png" height = 500 >
</script></section><section data-markdown><script type="text/template">
### The design matrix

- N examples 
- described by D features
- NxD design matrix
- predict the value of N target values

<img src=https://i.stack.imgur.com/VZtEr.jpg height = 200>
</script></section><section data-markdown><script type="text/template">
### Parametric vs. Non-parametric models

- parametric: closed form, #parameters is fixed
	- fast to compute
	- strong assumptions
- non-parametric, #parameters grows with amount of (training) data
	- flexible
	- often computationally intractable
</script></section><section data-markdown><script type="text/template">
### Parametric vs. Non-parametric models - K-nearest neighbours

<img src=http://cs231n.github.io/assets/knn.jpeg >

- guaranteed error rate <= 2 Bayes error (for infinite training set)
- scales with the size of the training set 
</script></section><section data-markdown><script type="text/template">
### The curse of dimensionality

- Bellman 
- in high dimensions sampling effort grows exponentially
- (Euclidean) distances become meaningless 
	- ration between min and max distance approaches 1 with D -> Inf
- growing hypercube to collect data: e(f) = f^(1/D)
	- if we want f = 0.1 (10% of data) in D=10 dimensions, side length e = 0.8 (80% of max side-length)
</script></section><section data-markdown><script type="text/template">
### Parametric models

- to escape the curse of dimensionality we can make assumptions about the data distribution (inductive bias)
- parametric models with fixed number of parameters
</script></section><section data-markdown><script type="text/template">
### Parametric models: regression

- linear regression
- $y(x,w) = w_0 + w_1 x_1 + ... + w_D x_D$

<img src=https://cdn-images-1.medium.com/max/600/1*iuqVEjdtEMY8oIu3cGwC1g.png height = 400 style="background-color:white;">
</script></section><section data-markdown><script type="text/template">
### Parametric models: classification

- logistic regression
- linear regression + sigmoid nonlinearity
- $ y(\Phi) = \sigma (\vec{w}^T \Phi)$

<img src=http://www.saedsayad.com/images/LogReg_1.png>
</script></section><section data-markdown><script type="text/template">
### Generalisation 

- how is performance on **unseen data** ?
- using a **test set** separate from training set
</script></section><section data-markdown><script type="text/template">
### Overfitting

- if we model every minute variation in the training data, we are likely to fit the noise as well
- less accurate prediction for future data
- perfect fit of training data = bad generalisation to unseen data
</script></section><section data-markdown><script type="text/template">
### Model selection
- how to select a good model (e.g. the k in kNN)?
	- misclassification rate on training set
	- but: we care about generalisation error: misclassification rate on test set
	- out-of sample error 
</script></section><section data-markdown><script type="text/template">
<img src=https://pbs.twimg.com/media/CnRKSa8UsAAR2JC.jpg>
</script></section><section data-markdown><script type="text/template">
### the test set

- during training we don't have access to test set:
	- split training data into training set and validation set
		- e.g. 80/20
	
</script></section><section data-markdown><script type="text/template">
### Cross Validation

- cross validation (k folds)

<img src= https://cdn-images-1.medium.com/max/1600/1*J2B_bcbd1-s1kpWOu_FZrg.png height = 500>
</script></section><section data-markdown><script type="text/template">
## model selection

- optimise over models (or hyperparameters)
- use a validation set
</script></section><section data-markdown><script type="text/template">
## model selection 

- training set
- validation set
- test set (don't touch!)
</script></section><section data-markdown><script type="text/template">
## No free lunch theorem 

> All models are wrong, but some models are useful. — George Box (Box and Draper 1987, p424).

- *no free lunch* theorem (Wolpert 1996)
	- no universally best model across all problems
	- assumptions that works well in one domain often fail in another
</script></section><section data-markdown><script type="text/template">
### The unreasonable effectiveness of data

- simple models with lots of data will beat sophisticated models with few data
- try to use unsupervised learning if you can
</script></section></section><section ><section data-markdown><script type="text/template">
# Summary
</script></section><section data-markdown><script type="text/template">
- types of ML methods
	- supervised, unsupervised
	- classification, regression
	- parametric vs. non-parametric 
- basic concepts
	- input, features, targets ...  
	- model selection
		- generalisation and overfitting
		- training set, test set, validation set 
</script></section></section><section ><section data-markdown><script type="text/template">
# books 
</script></section><section data-markdown><script type="text/template">
- Bishop - Pattern Recognition and Machine Learning

<img src=https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/Springer-Cover-Image.jpg height = 500>
</script></section><section data-markdown><script type="text/template">
- Murphy - Machine Learning: A Probabilistic Perspective

<img src=https://mitpress.mit.edu/sites/default/files/9780262018029.jpg height = 500>
</script></section><section data-markdown><script type="text/template">
- Goodfellow, Courville, Bengio - Deep Learning

<img src=https://images.gr-assets.com/books/1478212695l/30422361.jpg height = 400>
</script></section><section data-markdown><script type="text/template">
- Geron - Hands-on Machine Learning with Scikit-Learn and TensorFlow

<img src=https://covers.oreillystatic.com/images/0636920052289/lrg.jpg height = 500>
</script></section><section data-markdown><script type="text/template">
- Abu-Mostafa, Magdon-Ismail, Lin - Learning From Data

<img src=http://amlbook.com/images/front.jpg height = 500>
</script></section><section data-markdown><script type="text/template">
- Hastie, Tibshirani, Friedman - The Elements of Statistical Learning

<img src=https://web.stanford.edu/~hastie/ElemStatLearn/CoverII_small.jpg height = 500>

---- 

The end</script></section></section></div>
        </div>

        <script src="./lib/js/head.min.js"></script>
        <script src="./js/reveal.js"></script>

        <script>
            function extend() {
              var target = {};
              for (var i = 0; i < arguments.length; i++) {
                var source = arguments[i];
                for (var key in source) {
                  if (source.hasOwnProperty(key)) {
                    target[key] = source[key];
                  }
                }
              }
              return target;
            }

            // Optional libraries used to extend on reveal.js
            var deps = [
              { src: './lib/js/classList.js', condition: function() { return !document.body.classList; } },
              { src: './plugin/markdown/marked.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
              { src: './plugin/markdown/markdown.js', condition: function() { return !!document.querySelector('[data-markdown]'); } },
              { src: './plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
              { src: './plugin/zoom-js/zoom.js', async: true },
              { src: './plugin/notes/notes.js', async: true },
              { src: './plugin/math/math.js', async: true }
            ];

            // default options to init reveal.js
            var defaultOptions = {
              controls: true,
              progress: true,
              history: true,
              center: true,
              transition: 'default', // none/fade/slide/convex/concave/zoom
              dependencies: deps
            };

            // options from URL query string
            var queryOptions = Reveal.getQueryHash() || {};

            var options = {};
            options = extend(defaultOptions, options, queryOptions);
        </script>


        <script>
            Reveal.initialize(options);
        </script>
    </body>
</html>
